# 人工智能 artificial intelligence (Selmer Bringsjord and Naveen Sundar Govindarajulu)

_首次发布于 2018 年 7 月 12 日_

人工智能（AI）是致力于构建人造动物（或至少在适当的情境下_看起来_像动物的人造生物）的领域，对许多人来说，也包括构建人造人（或至少在适当的情境下_看起来_像人的人造生物）\[[1](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-1)]。这些目标立即使得人工智能成为许多哲学家感兴趣的学科，并且这一点已经得到证实（例如），许多哲学家积极尝试证明这些目标实际上是不可/可达到的。在建设性方面，人工智能中使用的许多核心形式和技术都源自哲学，并且仍然在哲学中得到广泛使用和完善：一阶逻辑及其扩展；适用于建模信念态度和道义推理的内涵逻辑；归纳逻辑、概率论和概率推理；实践推理和规划等等。鉴于此，一些哲学家将人工智能研究和开发视为哲学的一部分。

在本文中，简要回顾了人工智能的历史，讨论了该领域的定义，并提供了该领域的概述。此外，还通过具体例子讨论了哲学人工智能（作为哲学的一部分进行的人工智能研究和开发）和人工智能的哲学。文章以一些关于人工智能未来的_必要的_推测性评论结束。

* [1. 人造物的历史](https://plato.stanford.edu/entries/artificial-intelligence/#HistAI)
* [2. 人造物究竟是什么？](https://plato.stanford.edu/entries/artificial-intelligence/#WhatExacAI)
* [3. AI 的方法](https://plato.stanford.edu/entries/artificial-intelligence/#ApprAI)
* [3.1 智能代理的连续性](https://plato.stanford.edu/entries/artificial-intelligence/#InteAgenCont)
* [3.2 基于逻辑的人造智能：一些手术要点](https://plato.stanford.edu/entries/artificial-intelligence/#LogiBaseAISomeSurgPoin)
* [3.3 非逻辑主义人造智能：摘要](https://plato.stanford.edu/entries/artificial-intelligence/#NonLogiAISumm)
* [3.4 人造物超越范式的冲突](https://plato.stanford.edu/entries/artificial-intelligence/#AIBeyoClasPara)
* [4. 人造物的爆炸性增长](https://plato.stanford.edu/entries/artificial-intelligence/#ExplGrowAI)
* [4.1 机器学习中的开花](https://plato.stanford.edu/entries/artificial-intelligence/#BlooMachLear)
* [4.2 神经计算技术的复兴](https://plato.stanford.edu/entries/artificial-intelligence/#ResuNeurTech)
* [4.3 概率技术的复兴](https://plato.stanford.edu/entries/artificial-intelligence/#ResuProbTech)
* [5. 野外中的人造物](https://plato.stanford.edu/entries/artificial-intelligence/#AIWild)
* [6. 道德人造物](https://plato.stanford.edu/entries/artificial-intelligence/#MoraAI)
* [7. 哲学人造物](https://plato.stanford.edu/entries/artificial-intelligence/#PhilAI)
* [8. 人造物的哲学](https://plato.stanford.edu/entries/artificial-intelligence/#PhilArtiInte)
* [8.1 “强人工智能”与“弱人工智能”](https://plato.stanford.edu/entries/artificial-intelligence/#StroVersWeakAI)
* [8.2 对“强人工智能”的中文房间论证](https://plato.stanford.edu/entries/artificial-intelligence/#ChinRoomArguAgaiStroAI)
* [8.3 对“强人工智能”的哥德尔论证](https://plato.stanford.edu/entries/artificial-intelligence/#GodeArguAgaiStroAI)
* [8.4 人工智能哲学的其他主题和阅读材料](https://plato.stanford.edu/entries/artificial-intelligence/#AddiTopiReadPhilAI)
* [9. 未来](https://plato.stanford.edu/entries/artificial-intelligence/#Futu)
* [参考文献](https://plato.stanford.edu/entries/artificial-intelligence/#Bib)
* [学术工具](https://plato.stanford.edu/entries/artificial-intelligence/#Aca)
* [其他互联网资源](https://plato.stanford.edu/entries/artificial-intelligence/#Oth)
* [人造物的在线课程](https://plato.stanford.edu/entries/artificial-intelligence/#OnliCourAI)
* [相关条目](https://plato.stanford.edu/entries/artificial-intelligence/#Rel)

***

## 1. 人工智能的历史

人工智能（AI）领域正式始于 1956 年，由一个小而如今著名的[DARPA](http://www.darpa.mil/)赞助的夏季会议在新罕布什尔州汉诺威的达特茅斯学院举行。（这次会议的 50 周年庆祝活动，[AI@50](https://web.archive.org/web/20200315115548/http://www.dartmouth.edu/\~ai50/homepage.html)，于 2006 年 7 月在达特茅斯举行，其中五位原始参与者回到了会场。\[2]这次历史性会议的内容在本文的最后一节中有所涉及。）会议共有十位思想家参加，包括约翰·麦卡锡（他在 1956 年在达特茅斯工作）、克劳德·香农、马文·明斯基、亚瑟·塞缪尔、特伦查德·摩尔（显然是原始会议上唯一的记录者）、雷·所罗门诺夫、奥利弗·塞尔弗里奇、艾伦·纽厄尔和赫伯特·西蒙。从我们现在的角度来看，进入新千年的开始，达特茅斯会议因许多原因而令人难忘，其中包括以下两点：一是在那里创造了“人工智能”这个术语（尽管有些与会者不喜欢，比如摩尔），但这个术语已经牢固地确立下来；二是纽厄尔和西蒙展示了一个被与会者（实际上是几乎所有在会议结束后不久得知和了解这个项目的人）认为是一个了不起的成就的程序——逻辑理论家（LT）。LT 能够证明命题演算中的基本定理。

尽管“人工智能”这个术语是在 1956 年的会议上首次出现，但 AI 领域的运作定义（即由以某种方式思考和行动的从业者组成的领域）在 1956 年之前就已经存在。例如，在 1950 年的一篇著名的《心智》论文中，艾伦·图灵认为“机器能思考吗？”这个问题（图灵在这里谈论的是标准计算机：能够计算自然数（或其对、三元组等）到图灵机或等价物能处理的自然数的机器）应该被替换为“机器能否在语言上与人类无法区分？”他具体提出了一个测试，即“[图灵测试](https://plato.stanford.edu/entries/turing-test/)”（TT），如今已经被广泛知晓。在 TT 中，一名女性和一台计算机被隔离在密封的房间里，一个人类评委在不知道哪个房间有哪个参赛者的情况下，通过电子邮件（实际上是通过电传打字机，使用的是原始术语）向两个参赛者提问。如果评委根据回答的内容无法在判断哪个房间有哪个参赛者时做出更好的选择，我们说这台计算机已经通过了 TT。在这个意义上，通过 TT 来实现语言上的无法区分性。稍后，我们将讨论 TT 在定义 AI 方面所起的作用，以及它在 AI 领域的基础讨论中所起的作用，正如（Moor 2003）所证实的那样。事实上，TT 继续被用来定义这个领域，就像尼尔森（1998）在他的教科书中所表达的那样，AI 就是致力于构建能够通过这个测试的人造物的领域。通过梦想工程一台能够通过 TT 的计算机，或者争议围绕着已经通过了 TT 的声明，这种能量比以往任何时候都更加强大，读者只需通过互联网搜索字符串来验证。

图灵测试通过

为了找到最新的努力来实现这个梦想，以及试图（有时是由哲学家进行的）揭穿某些这样的尝试已经成功的说法。

回到历史记录的问题上，即使有人加强了 AI 起源于 1956 年的会议的说法，并附加了“人造智能”指的是一种实际的工程追求的条件（在这种情况下，图灵的哲学讨论，尽管有人要求制造一个孩子机器，但并不能算作 AI 本身），我们仍然必须面对这样一个事实：图灵，以及许多前辈，确实试图构建智能的人造物。在图灵的情况下，在可编程计算机出现之前，这种构建是出奇地被人们理解的：图灵在计算机运行此类程序之前，就为下棋编写了一个程序，他自己严格按照代码进行操作。他在 1950 年之前就做到了这一点，远在 Newell（1973）在印刷品上对构建一个出色的下棋计算机的可能性进行深思之前。\[[6](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-6)]

从哲学的角度来看，将机械智能的系统研究视为有意义和富有成效的，与产生计算机科学的具体逻辑形式（例如一阶逻辑）和问题（例如“决策问题”）是分开的。因此，无论是 1956 年的会议，还是图灵的《心智》论文，都远远不能被视为 AI 的起点。这很容易理解。例如，笛卡尔在图灵出生很久之前就提出了 TT（当然不是以这个名字），以下是相关段落：\[[7](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-7)]

如果有一种机器与我们的身体相似，并在道德上尽可能地模仿我们的行为，那么我们总是有两个非常确定的测试来认识到，尽管如此，它们并不是真正的人类。第一个测试是，它们永远无法像我们一样使用语言或其他符号，将我们的思想记录下来以便他人受益。因为我们可以很容易地理解一台机器被构造成可以发出声音，甚至对其身体上的某种行动作出一些反应，从而改变其器官的状态；例如，如果它被触摸到某个特定部位，它可能会问我们想对它说什么；如果在另一个部位被触摸，它可能会呼喊自己受伤了，等等。但它从来没有以各种方式安排自己的言辞，以便适当地回应在它面前说的一切，就像即使是最低级别的人类也能做到的那样。第二个区别是，虽然机器可以像我们中的任何人一样或者可能比我们更好地执行某些事情，但它们在其他方面无疑会表现不足，通过这种方式我们可以发现它们并不是出于知识而行动，而只是出于器官的安排。因为理性是一种通用工具，可以适用于所有情况，而这些器官在每个特定的行动中都需要一些特殊的适应。由此可见，在任何一台机器中，要使其能够像我们的理性使我们行动一样在生活的所有事件中行动，这是在道德上不可能的。（笛卡尔，1637 年，第 116 页）

目前，笛卡尔的观点无疑占据了上风。\[8] 图灵预测他的测试将在 2000 年通过，但新千年伊始全球的烟花已经消失殆尽，最善辩的计算机仍然无法与一个聪明的幼儿进行有意义的辩论。此外，虽然在某些专注的领域，机器的表现超过了人类的思维（例如，IBM 著名的深蓝在国际象棋中战胜了加里·卡斯帕罗夫；最近，人工智能系统在其他游戏中也取得了胜利，例如《危险边缘》和围棋，稍后将会详述），但人类（笛卡尔式的）具有在几乎任何领域培养自己专业知识的能力。（如果向深蓝或任何现有的继任者宣布国际象棋不再是首选游戏，而是一种以前从未玩过的国际象棋变种，那么这台机器将被智力平均的儿童轻松击败，这些儿童没有国际象棋的专业知识。）人工智能简单地没有成功创建出普遍的智能；它甚至没有成功制造出一个表明它最终将创造出这样一种东西的人造物。

但是 IBM Watson 在_Jeopardy!_ 游戏节目中的著名胜利呢？\[[9](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-9)] 这显然是机器在人类的“主场”上战胜人类，因为_Jeopardy!_ 提供了一个跨越多个领域的人类水平的语言挑战。事实上，在许多人工智能专家中，Watson 的成功被认为比 Deep Blue 的成功更令人印象深刻，原因有很多。其中一个原因是，虽然国际象棋通常被认为从形式计算的角度来看是被很好理解的（毕竟，众所周知国际象棋存在一种完美的策略），但在开放领域的**问答**（QA）中，就像在任何重要的自然语言处理任务中一样，对于从形式上来说要解决的问题没有共识。简而言之，问答（QA）是读者所想的那样：人们向机器提问，然后得到一个答案，这个答案必须通过某种“重要”的计算过程产生。（有关 QA 历史上的概述，请参见 Strzalkowski＆Harabagiu（2006））更准确地说，对于从形式上来说，问答能力计算的基本功能没有共识。这种缺乏共识自然而然地源于对于从形式上来说自然语言是什么没有共识。\[[10](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-10)]尽管存在这种模糊性，并且几乎普遍认为开放领域的问答问题将在十年甚至更长时间内无法解决，Watson 在这个星球上决定性地击败了两位顶级的_Jeopardy!_ 冠军。在比赛中，Watson 不仅需要回答需要掌握简单事实的问题（**问题**1），还需要一定程度的基本推理（以时间推理的形式）和常识（**问题**2）：

**问题**1：美国连续两任总统的名字相同。

**问题**2：1898 年 5 月，葡萄牙庆祝这位探险家抵达印度的 400 周年纪念。

尽管沃森在_危险边缘_风格的问答游戏中明显优于人类（新的人类_危险边缘_大师可能会出现，但对于国际象棋来说，人工智能现在认为，进行第二轮 IBM 级别的投资将击败新的人类对手），但这种方法对笛卡尔所描述的 NLP 挑战不起作用；也就是说，沃森无法即时对话。毕竟，有些问题不仅仅依赖于复杂的信息检索和机器学习，还依赖于当场的复杂推理。这样的问题可能涉及指代消解，需要更深入的常识性对时间、空间、历史、民间心理学等的理解。Levesque（2013）列举了一些令人震惊的简单问题，属于这一类别。（Marcus，2013，对 Levesque 的挑战进行了更广泛的解释。）沃森失败的另一类问题回答任务可以被描述为_动态_问题回答。这些问题的答案可能在提问时没有以文本形式记录，或者答案依赖于随时间变化的因素。以下是两个属于这一类别的问题（Govindarajulu 等人，2013）：

**问题**3：如果我有 4 个 foo 和 5 个 bar，并且 foo 和 bar 不是一样的，如果我得到 3 个刚好是 foo 的 baz，我会有多少个 foo？

**问题**4：IBM 在过去 60 天的交易中的夏普比率是多少？

紧随沃森的胜利之后，在 2016 年 3 月，[Google DeepMind 的 AlphaGo](https://www.deepmind.com/research/highlighted-research/alphago)在五场比赛中以四场胜利击败了围棋排名靠前的李世石。这被认为是人工智能领域的里程碑式成就，因为在人工智能界普遍认为计算机在围棋上的胜利至少还需要几十年的时间，部分原因是围棋中有效的走法序列数量远远超过国际象棋。\[11]虽然这是一个了不起的成就，但需要注意的是，尽管在大众媒体中广泛报道，AlphaGo 虽然无可争议地是一位出色的围棋选手，但仅仅是这样。例如，AlphaGo 和沃森都无法理解用简单明了的英语写成的围棋规则，并生成一个能够下围棋的计算机程序。有趣的是，在人工智能领域中有一个努力解决这个问题的狭义版本：在**通用游戏玩法**中，机器在开始玩游戏之前会得到一个全新游戏的描述（Genesereth 等人，2005）。然而，所讨论的描述是用形式语言表达的，机器必须通过这个描述来玩游戏。需要注意的是，这仍然远远不能理解用英语描述的简单游戏规则以至于能够玩游戏。

但是，如果我们从哲学的角度而不是从与之最密切相关的领域的角度来考虑人工智能的历史，会怎么样呢？这里指的是计算机科学。从这个角度来看，人工智能是否可以追溯到图灵之前？有趣的是，结果是一样的：我们发现人工智能的历史悠久，并且一直与哲学有着紧密的联系。这是因为计算机科学起源于逻辑和概率论，\[13]而逻辑和概率论又起源于（并且仍然与）哲学紧密相连。如今，计算机科学中充斥着逻辑；这两个领域是密不可分的。这种现象已经成为一个独立的研究对象（Halpern 等人，2001）。当我们谈论的不是传统逻辑，而是概率形式主义时，情况也并不不同，概率形式主义也起源于哲学，正如 Glymour（1992）所详细记载的那样。例如，在帕斯卡的头脑中诞生了一种严格计算概率的方法，条件概率（目前在人工智能中起着特别重要的作用），以及像[Pascal 的赌注](https://plato.stanford.edu/entries/pascal-wager/)这样富有哲学和概率论的论证，根据这个论证，不成为基督徒是不理性的。

现代人工智能的根源可以追溯到哲学，事实上，这些历史根源甚至比笛卡尔的时代更为久远。通过观察综合教材《人工智能：现代方法》（第二版）的巧妙而富有启示性的封面（第三版是当前版本），可以看出这一点。在人工智能界，这本教材简称为《AIMA2e》（Russell & Norvig, 2002）。

![AIMA2e 封面](https://plato.stanford.edu/entries/artificial-intelligence/2ebig.jpg)

_AIMA2e_（Russell & Norvig 2002）的封面

你在那里看到的是一些可能出现在某个想象中的人工智能研究者桌子上和周围的杂项纪念品。例如，如果你仔细看，你会特别看到：图灵的照片，透过窗户看到的大本钟（也许 R\&N 意识到了图灵曾经著名地认为具有通用图灵机能力的物理机器在物理上是不可能的：他开玩笑说它的大小必须和大本钟一样），亚里士多德的《动物运动论》中描述的规划算法，[弗雷格关于一阶逻辑的迷人符号表示法](https://plato.stanford.edu/entries/frege/)，刘易斯·卡罗尔（1958）对三段论推理的图示表示，拉蒙·卢尔（Ramon Lull）在他 13 世纪的《大艺术》中的概念生成轮，以及其他一些富有意味的物品（包括一个巧妙、递归且近乎自我恭维的触动，即《AIMA》本身的副本）。虽然这里的空间不足以建立所有的历史联系，但我们可以从这些物品的出现推断出（当然我们指的是古代的物品）：AI 确实非常非常古老。即使那些坚持认为 AI 至少在某种程度上是一个人造物建设企业的人也必须承认，鉴于这些物品，AI 是古老的，因为不仅仅是从智能在本质上是计算的角度进行理论化的观点可以追溯到人类历史的遥远过去：例如，卢尔的轮子标志着一种试图在计算中捕捉智能的尝试，而是在一个物理的人造物中_体现_了那种计算。\[[14](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-14)]

AIMA 现已进入第三版，对于对人工智能历史以及心灵哲学历史感兴趣的人来说，仔细研究第三版封面将不会让他们失望（第二版封面与第一版几乎完全相同）。（封面的所有元素，分别列出并加以注释，可以在[在线网页](http://aima.cs.berkeley.edu/cover.html)上找到。）第三版封面的一个重要添加是托马斯·贝叶斯的插图；他的出现反映了概率技术在人工智能中的近期流行，我们稍后会讨论。

关于人工智能历史的最后一点似乎值得一提。

普遍认为，现代人工智能的诞生主要归功于和通过现代高速数字计算机的出现。这种假设符合常识。毕竟，人工智能（以及在某种程度上与之相关的认知科学，特别是认知科学的计算认知建模子领域，该子领域致力于在计算机中实现人类认知的计算模拟）旨在在计算机中实现智能，因此可以合理地认为这样的目标与这些设备的出现密不可分。然而，这只是故事的一部分：回溯到图灵和其他（例如冯·诺依曼）负责第一台电子计算机的人。另一部分是，正如前面提到的，从历史上看，人工智能与推理（基于逻辑的推理以及处理不确定性的归纳/概率推理）有着特别紧密的联系。在格莱莫（1992）所讲述的这个故事中，对“什么是证明？”这个问题的探索最终得出了一个基于弗雷格的一阶逻辑（FOL）版本的答案：（有限的）数学证明由一系列从一个一阶逻辑公式到下一个公式的逐步推理组成。这个答案的明显延伸（并不是一个完整的答案，因为尽管常识认为，许多经典数学显然无法用 FOL 表达；即使是作为有限公式集来表达的皮亚诺公理也需要_S_OL）是说，不仅数学思维，而且一般思维都可以用 FOL 来表达。（这个延伸在信息处理心理学和认知科学开始之前就被许多逻辑学家考虑过，这是一些认知心理学家和认知科学家经常似乎忘记的事实。）今天，基于逻辑的人工智能只是人工智能的一部分，但关键是这一部分仍然存在（借助比 FOL 更强大但更复杂的逻辑），并且可以追溯到亚里士多德的三段论理论。在不确定推理的情况下，问题不是“什么是证明？”，而是诸如“根据某些观察和概率，相信什么是合理的？”这是在数字计算机出现之前很久就提出和解决的问题。

## 2. 人工智能到底是什么？

到目前为止，我们一直按照我们对人工智能（AI）的本质有着坚定而准确的把握来进行。但是，AI 到底是什么呢？哲学家们可以说比任何人都更清楚，精确地定义一个特定学科以满足所有相关方的要求（包括该学科内部的从业人员）是非常具有挑战性的。科学哲学家们确实提出了可信的关于构成某个科学和/或工程领域的一般形状和纹理的解释，但是物理学的确切定义是什么？生物学呢？哲学到底是什么？这些问题非常困难，甚至可能是永远无法回答的问题，尤其是如果目标是达成共识的定义。也许在明显的空间限制下，我们在这里能采取的最明智的做法是以简洁的形式呈现一些关于 AI 的提议性定义。我们确实包括了最近试图以详细而严谨的方式定义 AI 的尝试的一瞥（我们猜想这样的尝试对科学哲学家和对这个哲学子领域感兴趣的人可能会感兴趣）。

在他们之前提到的《AIMA》文本中，Russell 和 Norvig（1995 年，2002 年，2009 年）提供了一组在该领域中具有相当影响力的“什么是 AI？”问题的可能答案。这些答案都假设 AI 应该根据其目标来定义：一个候选定义的形式是“AI 是旨在构建...的领域”。这些答案都属于四种类型，沿着两个维度排列。一个维度是目标是匹配人类表现还是理想的理性。另一个维度是目标是构建能够推理/思考的系统，还是行动的系统。这种情况在下表中总结：

\| | **以人为本** | **理想理性** |

\| --- | --- | --- |

\| **基于推理的：** | 像人类一样思考的系统。 | 像人类一样理性思考的系统。 |

\| **基于行为的：** | 像人类一样行动的系统。 | 像理性行动的系统。 |

_根据 AIMA，人工智能的四个可能目标_

请注意，这四个可能性确实反映了（至少是相当大一部分的）相关文献。例如，哲学家约翰·霍根兰（1985 年）在他说到人工智能是“让计算机思考的令人兴奋的新尝试……以完全和字面意义上的方式成为有思想的机器”时，属于人类/推理象限。（迄今为止，这是最受欢迎的叙事并进行探索的象限。最近的《西部世界》电视剧就是一个有力的例子。）卢格尔和斯塔布菲尔德（1993 年）在他们写道：“涉及智能行为自动化的计算机科学分支。”时，似乎属于理想/行动象限。图灵最突出地占据了人类/行动的立场，只有那些能够像人类一样行动的系统才能通过他的测试。“理性思考”的立场由温斯顿（1992 年）辩护。（尽管断言这里给出的四个分类是穷尽的可能会有争议，但在目前的文献调查中，这样的断言似乎是相当合理的。）

重要的是要知道，在关注思考/推理系统与行动系统之间的对比时，正如我们所见，这是_AIMA_文本的核心，也是人工智能本身的核心，不应被解释为意味着 AI 研究人员将他们的工作仅仅归入这两个部分之一。那些更多或更少地专注于知识表示和推理的研究人员也非常愿意承认，他们正在研究（他们认为的）跨越推理/行动区别的更大系统家族中的一个核心组件或能力。最明显的例子可能来自于规划工作——这是一个传统上广泛使用表示和推理的 AI 领域。无论是好是坏，这项研究的大部分工作都是在抽象中进行的（体外，而不是体内），但参与其中的研究人员肯定打算或至少希望他们的工作结果能够嵌入到实际执行计划等任务的系统中。

那么，Russell 和 Norvig 自己呢？对于“什么是 AI？”这个问题，他们坚定地站在“以理性行动”阵营。事实上，可以说他们是这个答案的主要支持者，并且他们一直是非常成功的传道者。他们极具影响力的_AIMA_系列可以被视为一本书长的对理想/行动类别的辩护和规范。稍后我们将看一下 Russell 和 Norvig 如何以**智能代理**的术语来概述整个 AI 领域，智能代理是按照各种理性的理想标准行动的系统。但首先，让我们更仔细地看一下_AIMA_文本所基于的智能观。我们可以通过参考 Russell（1997）来做到这一点。在这里，Russell 将“什么是 AI？”的问题重新解释为“什么是智能？”的问题（假设我们对人造物有很好的理解），然后他将智能与**理性**等同起来。更具体地说，Russell 将 AI 视为致力于构建**智能代理**的领域，这些代理是根据来自外部环境的感知元组作为输入，并根据这些感知产生行为（动作）的函数。Russell 的整体观点如下：

!['感知历史'到'代理功能'到'行为'到'环境'到'感知历史'; 同样 '环境'到/从'状态历史'到'绩效度量'](https://plato.stanford.edu/entries/artificial-intelligence/rationality\*diagram\*3.png)

_罗素对智能/理性的基本图景_

让我们对这个图解进行一些解读，并首先看一下可以从中得出的**完美理性**的描述。在环境 E（来自环境类 E）中，代理的行为产生了该环境的一系列状态或快照。一个性能度量 U 评估这个序列；请注意上图中标有“性能度量”的方框。我们用 V(f,E,U)表示代理函数 f 在 E 上操作时根据 U 的_期望_效用。\[[16](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-16)]现在我们将完美理性的代理与代理函数进行了等同：

fopt=argmaxfV(f,E,U)(1)

根据上述方程，一个完全理性的代理可以被视为在考虑的环境中产生最大期望效用的函数 fopt。当然，正如 Russell 指出的那样，实际上很难构建完全理性的代理。例如，虽然很容易指定一个下棋无敌的算法，但实施这个算法是不可行的。在人工智能领域，传统做法是构建一些在 Russell 恰当的术语中称为“计算上理性”的程序：这些程序在无限快的情况下会产生完全理性的行为。在下棋的情况下，这意味着我们努力编写一个能够找到完美着法的算法，但我们添加了一些功能来限制对这个着法的搜索，以便在可接受的时间范围内进行游戏。

Russell 本人提倡一种新的人工智能智能/理性品牌，他称之为“有界最优性”。为了理解 Russell 的观点，首先我们按照他的观点引入一个区别：我们说代理有两个组成部分：一个程序和一个运行该程序的机器。我们用 Agent(P,M)表示由程序 P 在机器 M 上运行的代理函数。现在，让 P(M)表示所有可以在机器 M 上运行的程序 P 的集合。那么“有界最优”程序 Popt,M 就是：

Popt，M = argmaxP∈P（M）V（Agent（P，M），E，U）

你可以从任何数学理想化的角度理解这个方程。例如，机器可以被认为是图灵机减去指令（即，图灵机在这里仅被视为具有被分成方块的纸带，可以在上面写入、读取和擦除符号的读写头，以及控制单元，该单元在任何时候都处于有限数量的状态之一），而程序可以被认为是图灵机模型中的指令（根据机器所处的状态，告诉机器写入和擦除符号）。因此，如果你被告知必须在 22 个状态的图灵机的约束下进行“编程”，你可以在这些约束条件下寻找“最佳”程序。换句话说，你可以努力在 22 个状态的架构范围内找到最优的程序。因此，罗素（1997）认为，人工智能是致力于为智能代理创建在实施这些程序的机器上的时间和空间约束下的最优程序的领域。\[17]

读者一定注意到，在 Popt,M 的方程中，我们没有详细说明 E 和 U 以及方程(1)如何用于构建一个代理，如果环境 E 的类别非常普遍，或者真实环境 E 只是未知的。根据构建人造代理的任务，E 和 U 会有所不同。环境 E 和效用函数 U 的数学形式会因为不同的任务而大不相同，比如从国际象棋到 Jeopardy!。当然，如果我们设计一个全球智能的代理，而不仅仅是一个下棋的代理，我们可以只使用一对 E 和 U。如果我们正在构建一个普遍智能的代理，而不仅仅是一个擅长单一任务的代理，那么 E 会是什么样子呢？E 将是一个模型，不仅仅是一个单一游戏或任务的模型，而是包括许多游戏、任务、情境、问题等的整个物理-社会-虚拟宇宙。这个项目（至少目前）是无望困难的，因为显然我们离拥有这样一个全面的万物理论模型还差得很远。关于为这个问题提出的理论架构的进一步讨论，请参见《AIXI 架构补充》。

应该提到，“什么是人工智能？”这个问题有一个不同得多、更直接的答案。这个答案可以追溯到最初的达特茅斯会议的日子，由现代人工智能的创始人之一纽厄尔（1973）等人提出（请记住他参加了 1956 年的会议）；这个答案是：

人工智能是致力于构建智能的人造物的领域，其中“智能”通过智力测试（如韦氏成人智力量表）和其他心理能力测试（包括机械能力、创造力等测试）来操作化。

上述定义可以被视为对 Russell 和 Norvig 的四个可能目标的具体版本的完全规定。虽然现在很少有人意识到这一点，但这个答案曾经被认真对待过一段时间，并且实际上是 AI 历史上最著名的项目之一的基础：Evans（1968）的 ANALOGY 程序，该程序解决了许多智力测试中出现的几何类比问题。Bringsjord 和 Schimanski（2003）试图严格定义这种被遗忘的 AI 形式（他们称之为“心理测量 AI”），并从 Newell 和 Evans 的时代中复兴它\[另见（Bringsjord 2011）]。目前已经在进行的尝试中，已经投入了相当大的私人投资，即所谓的[Project Aristo](http://allenai.org/aristo/)，旨在构建一个“数字亚里士多德”，即一台能够在美国高中生所面对的 AP 考试等标准化测试中表现出色的机器（Friedland 等人，2004 年）。在这个方向上，[Allen Institute for Artificial Intelligence](http://allenai.org/)的研究人员也在进行着活跃的工作\[[18](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-18)]。此外，西北大学的研究人员还在人工智能和机械能力测试之间建立了联系（Klenk 等人，2005 年）。

最后，就像任何学科一样，要真正了解这门学科需要你至少在某种程度上深入学习和实践，或者至少深入阅读。二十年前，这样的深入学习还是相对容易的。而今天，由于构成人工智能的内容已经迅速增长，这样的深入学习（或者至少是之后的学习）变得更加具有挑战性。

## 3. 人工智能的方法论

有许多方法可以“划分”人工智能。迄今为止，总结该领域最明智和最有成效的方法是再次参考《人工智能：一种现代方法》（_AIMA_）的文本，因为它对该领域进行了全面的概述。

### 3.1 智能代理的连续性

正如 Russell 和 Norvig（2009）在《AIMA》的前言中告诉我们的那样：

> 主要的统一主题是智能代理的概念。我们将人工智能定义为研究从环境中接收感知并执行动作的代理。每个这样的代理都实现了一个将感知序列映射到动作的函数，我们涵盖了表示这些函数的不同方式...（Russell＆Norvig 2009，vii）

基本图像可以用以下图示来概括：

![具有传感器和执行器的代理接收感知并执行动作的图示](https://plato.stanford.edu/entries/artificial-intelligence/agent-environment.png)

_智能代理的印象主义概述_

_AIMA_的内容基本上是从充实这个图像中得出的；也就是说，上面的图像对应着智能代理实现的整体功能的不同表示方式。从最弱的代理到更强大的代理，存在着一种递进的关系。下面的图像给出了本书早期讨论的一种简单代理的高层视图。（尽管简单，这种代理对应着 Rodney Brooks 在 1991 年设计和实现的无表示代理的架构。）

![没有对世界的内部模型进行交互的简单反射型智能体](https://plato.stanford.edu/entries/artificial-intelligence/simple-reflex-agent.png)

_一个简单的反射型智能体_

随着书的进展，代理人变得越来越复杂，并且它们所代表的功能的实现越来越多地依赖于目前人工智能所能提供的资源。下图概述了一个比简单的反射型代理人更聪明的代理人。这个更聪明的代理人具有内部建模外部世界的能力，因此不仅仅受限于当前可以直接感知的事物。

![具有世界模型的反射型代理人与环境互动](https://plato.stanford.edu/entries/artificial-intelligence/reflexstate-agent.png)

_一个更复杂的反射型智能体_

《AIMA》分为七个部分。当读者逐步阅读这些部分时，她将会了解到每个部分讨论的智能体所具备的能力。第一部分是对基于智能体的视角的介绍。第二部分关注的是赋予智能体在明确定义的环境中提前几步思考的能力。这里的例子包括能够成功玩完全信息游戏（如国际象棋）的智能体。第三部分涉及具有陈述性知识并能够以大多数哲学家和逻辑学家熟悉的方式进行推理的智能体（例如，基于知识的智能体推断出应采取哪些行动来实现其目标）。本书的第四部分通过以概率方式进行推理，为智能体提供了处理不确定性的能力。\[[19](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-19)]第五部分赋予智能体学习的能力。下图显示了学习智能体的整体结构。

![能够与环境互动学习的代理人](https://plato.stanford.edu/entries/artificial-intelligence/learning-agent.png)

_一个学习代理人_

最后一组赋予代理人的能力使他们能够进行交流。这些能力在第六部分中有所涉及。

那些耐心地跟随着越来越聪明的代理人的整个进化过程的哲学家们无疑会在到达第七部分的尽头时问道，是否有什么遗漏了。总体上，我们是否给予了足够的东西来构建一个人造人，还是只有足够的东西来构建一个简单的动物？这个问题隐含在 Charniak 和 McDermott（1985）的以下论述中：

人工智能的最终目标（我们离实现这个目标还有很长的路要走）是构建一个人，或者更谦逊地说，一个动物。（Charniak＆McDermott 1985，7）

值得赞扬的是，Russell＆Norvig 在《AIMA》的第 27 章“AI：现在和未来”中，至少在某种程度上考虑了这个问题。\[] 他们这样做是通过考虑一些迄今尚未解决的人工智能挑战来实现的。其中一个挑战由 R＆N 描述如下：

机器学习在构建高于输入词汇的抽象层次的新表示问题上进展甚微。例如，在计算机视觉中，如果强制代理从像素作为输入表示开始学习，那么学习诸如教室和餐厅等复杂概念将变得不必要地困难；相反，代理需要能够首先形成中间概念，如桌子和托盘，而无需明确的人类监督。类似的概念也适用于学习行为：在许多计划中，喝茶是一个非常重要的高级步骤，但它如何进入最初只包含更简单的动作（如举手和吞咽）的动作库中呢？也许这将包括深度置信网络——具有多层隐藏变量的贝叶斯网络，如 Hinton 等人（2006）、Hawkins 和 Blakeslee（2004）以及 Bengio 和 LeCun（2007）的工作所示。...除非我们理解这些问题，否则我们将面临一个艰巨的任务，即通过手工构建大型常识知识库，而这种方法迄今为止并不成功。（Russell＆Norvig 2009，第 27.1 章）

尽管在解决这一挑战方面已经取得了一些进展（以深度学习或表示学习的形式），但这个具体的挑战实际上只是人工智能必须最终设法攀登的一系列令人眼花缭乱的高山之前的一个小山丘。其中之一，简单地说，就是阅读。尽管如前所述，《AIMA》的第五部分致力于机器学习，但就目前而言，人工智能在阅读学习的机械化方面几乎没有任何进展。然而，当你仔细思考时，阅读可能是你目前学习的主要方式。想想你此刻正在做什么。很可能你正在阅读这句话，因为之前你设定了一个目标，即了解人工智能领域。然而，《AIMA》第四部分提供的学习的形式模型（这些模型是人工智能中所使用的全部模型）无法应用于阅读学习。这些模型都从基于函数的学习观点开始。根据这个观点，学习几乎总是基于一组有限的配对产生一个底层函数 f。

{⟨x1,f(x1)⟩,⟨x2,f(x2)⟩,…,⟨xn,f(xn)⟩}.

例如，考虑接收由 1、2、3、4 和 5 组成的输入，以及相应的范围值 1、4、9、16 和 25；目标是“学习”从自然数到自然数的底层映射。在这种情况下，假设底层函数是 n2，并且您确实“学会”了它。虽然这种狭窄的学习模型可以应用于许多过程，但阅读过程不是其中之一。通过阅读学习不能（至少在可预见的未来）被建模为推测出产生参数-值对的函数。相反，只有当您的知识以正确的方式增加，并且这些知识使您能够产生行为以确认对所讨论的主题领域的充分掌握时，您对人工智能的阅读才能产生回报。这种行为可以从正确回答和证明与人工智能相关的测试问题，到产生一个强大、有说服力的演示或论文，以显示您的成就。

关于机器阅读，有两点值得注意。首先，并不是所有读者都清楚阅读是智能的核心能力。这种核心性源于智能需要广博的知识。我们没有其他办法将系统性的知识输入到系统中，除了从文本中获取，无论是网络上的文本，图书馆中的文本，报纸等等。你甚至可以说，人工智能的一个大问题是机器相比人类实际上并不知道太多。这只能是因为人类阅读（或听：不识字的人可以听到文本被朗读并通过这种方式学习）。要么机器通过人类手动编码和插入知识来获得知识，要么通过阅读和听力。这些都是铁一般的事实。（当然，我们暂且不谈超自然的技术。有趣的是，图灵似乎并没有忽略：他似乎认为超感知应该与心智和机器的能力讨论在一起。参见图灵，1950 年。）\[23]

现在是第二点。会阅读的人类无一例外地也学会了一门语言，并且学习语言已经按照上面概述的基于功能的方法进行建模（Osherson 等人，1986 年）。然而，这并不意味着一个能够阅读的人工智能代理，至少在相当程度上，必须真正学会一门自然语言。人工智能首要关注的是工程计算人造物，使其能够通过某种测试（是的，有时这个测试来自人类领域），而不是这些人造物是否以与人类情况相匹配的方式处理信息。在设计一台能够阅读的机器时，是否需要赋予该机器人类水平的语言能力是一个实证问题，随着时间的推移和工程的进行，我们无疑会看到这个问题得到解决。

面对 AI 的另外两座高山是主观意识和创造力，然而似乎这些巨大的挑战是该领域显然尚未应对的。对于许多心灵哲学家和神经科学家来说，对于心灵现象来说，这些现象在《人工智能哲学》中根本没有提及。例如，意识只是在《人工智能哲学》中简单提及，但主观意识是我们生活中最重要的事情 - 实际上，我们之所以希望继续生活，是因为我们希望继续享受某些类型的主观状态。此外，如果人类的思维是进化的产物，那么显然现象意识具有巨大的生存价值，并且对于一个旨在具有至少与我们自己的大脑相匹配的第一批具有行为能力的机器人来说，它将是极大的帮助（猎人采集者；参见 Pinker 1997）。当然，主观意识在认知心理学和计算认知建模的姊妹领域中也大多缺失。我们在下面的《人工智能哲学》部分讨论了其中的一些挑战。有关认知科学的类似挑战的列表，请参阅相关的《认知科学条目》部分。

对于一些读者来说，指出主观意识是 AI 尚未解决的一个重大挑战，可能至少看起来是有争议的。这些读者可能认为，指出这个问题是通过一种独特的哲学视角来看待 AI，而且是一种有争议的哲学立场。

但正如其文献所清楚表明的那样，人工智能通过观察动物和人类，并从中挑选出卓越的心智能力，然后看看这些能力是否可以被机械化来衡量自己。可以说，对人类来说最重要的能力（体验能力）在大多数人工智能研究者的目标清单上根本找不到。这可能有一个很好的理由（也许是因为没有现成的形式化方法），但无可否认的是，这种情况确实存在，并且从人工智能如何衡量自己的角度来看，这是令人担忧的。

至于创造力，令人惊讶的是，在《人工智能：一种现代方法》中，我们最称赞的能力在其中根本找不到。就像在（Charniak＆McDermott 1985）中无法在索引中找到“神经”一样，在《人工智能：一种现代方法》的索引中也找不到“创造力”。这特别奇怪，因为许多人工智能研究者实际上已经在创造力上进行了研究（尤其是那些来自哲学领域的研究者；例如，Boden 1994，Bringsjord＆Ferrucci 2000）。

虽然关注点一直在_AIMA_上，但它的任何对应物都可以被使用。以尼尔斯·尼尔森（Nils Nilsson）的《人工智能：一种新综合》（Artificial Intelligence: A New Synthesis）为例。与_AIMA_一样，这里的一切都围绕着从最简单的代理人（在尼尔森的案例中是_反应性代理人_）逐渐发展到具有越来越多区分人类的能力的代理人。积极的读者可以验证尼尔森的书的主要部分与_AIMA_之间存在着惊人的相似之处。此外，尼尔森像罗素和诺维格一样，忽略了现象意识、阅读和创造力。这三者甚至没有被提及。同样，Luger（2008）最近的一本综合性人工智能教材也遵循了相同的模式。

最后一个要点来结束本节。认为 AI 教材的结构具有一定的必然性似乎是相当合理的，而表面上的原因可能相当有趣。在个人交谈中，吉姆·亨德勒（Jim Hendler）是一位著名的 AI 研究人员，他是语义网（Berners-Lee，Hendler，Lassila 2001）背后的主要创新者之一，语义网是一种正在开发中的“AI-ready”版本的万维网，他曾表示，在教授 AI 导论时，这种必然性可以很容易地展示出来；以下是具体方法。首先询问学生们对 AI 的理解。不可避免地，许多学生会自愿表示 AI 是致力于构建具有智能的人造生物的领域。接下来，要求举出智能生物的例子。学生们总是给出一个连续的例子：简单的多细胞生物、昆虫、啮齿动物、低级哺乳动物、高级哺乳动物（最终是大猩猩），最后是人类。当要求学生描述他们所引用的生物之间的区别时，他们最终实际上描述了从简单代理人到具有我们（例如）交流能力的代理人的过程。这个过程构成了每本综合性 AI 教材的骨架。为什么会发生这种情况？答案似乎很明确：这是因为我们无法抵制以我们熟悉的现有生物的能力来构思 AI。至少目前来说，人类和只具有部分人性的生物是-重申一遍-衡量 AI 的标准。\[25]

### 3.2 基于逻辑的人工智能：一些手术要点

基于经典演绎逻辑的推理是单调的；也就是说，如果Φ⊢ϕ，那么对于所有的ψ，Φ∪{ψ}⊢ϕ。常识推理是非单调的。虽然你可能基于推理相信你的房子仍然屹立不倒，但如果在工作时你在电脑屏幕上看到一场巨大的龙卷风正在你房子所在的地方移动，你会放弃这种信念。新信息的添加导致先前的推断失败。在已成为人工智能基础的更简单的例子中，如果我告诉你 Tweety 是一只鸟，你会推断 Tweety 会飞，但如果我随后告诉你 Tweety 是一只企鹅，这个推断就会消失，这也是应该的。非单调（或可推翻的）逻辑包括旨在捕捉这些例子背后机制的形式化方法。请参阅关于[逻辑和人工智能](https://plato.stanford.edu/entries/logic-ai/)的单独条目，该条目侧重于非单调推理以及关于时间和变化的推理。它还提供了基于逻辑的人工智能早期历史的概述，清楚地说明了创立这一传统的人的贡献（例如，John McCarthy 和 Pat Hayes；请参阅他们的开创性论文 1969 年）。

逻辑 AI 的形式化和技术已经达到了令人印象深刻的成熟水平 - 以至于在各种学术和企业实验室中，这些形式化和技术的实现可以用于开发强大的现实世界软件。强烈建议对这些领域感兴趣的读者参考（Mueller 2006），该书提供了非单调推理（具体形式为环绕）以及关于情境和事件演算中的时间和变化的综合覆盖。 （前者演算也由 Thomason 引入。在第二个演算中，包括时间点在内，还有其他内容。）（Mueller 2006）的另一个好处是使用的逻辑是多重排序的一阶逻辑（MSL），这种逻辑具有统一的能力，许多技术哲学家和逻辑学家（Manzano 1996）都会了解和欣赏。

现在我们转向 AI 中的另外三个重要主题。它们是：

1. 逻辑主义人工智能的总体方案，处于构建智能人造物的尝试背景下。
2. 通用逻辑与日益加强的互操作性追求。
3. 一种可以称之为**编码降低**的技术，可以使机器能够高效地推理知识，如果不进行编码降低，推理过程将导致瘫痪的低效率。

这三个方面按顺序进行讨论，从第一个开始。

在代理为基础的方案下，可以在（Lenat 1983，Lenat＆Guha 1990，Nilsson 1991，Bringsjord＆Ferrucci 1998）中找到关于逻辑主义 AI 的详细说明\[[26](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-26)]。核心思想是智能代理以某种逻辑系统（例如，一阶逻辑）的公式形式接收来自外部世界的感知，并根据这些感知和其知识库推断出应该执行哪些操作以实现代理的目标。（这当然是一种野蛮的简化。来自外部世界的信息以公式的形式进行编码，并且实现此功能的转换器可能是代理的组成部分。）

为了澄清一些事情，我们简要考虑与任意**逻辑系统**LX 相关的逻辑主义观点\[[27](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-27)]。通过适当设置 X，我们可以得到一个特定的逻辑系统。一些例子：如果 X = I，则我们有一个在 FOL 级别上的系统\[遵循模型理论的标准符号法;参见（Ebbinghaus et al. 1984）]。LII 是二阶逻辑，而 LωIω是一个“小系统”的无穷逻辑（允许可数无穷的合取和析取）。这些逻辑系统都是**外延的**，但也有**内涵的**。例如，我们可以有与标准命题模态逻辑（Chellas 1980）中看到的逻辑系统相对应的逻辑系统。许多哲学家熟悉的一种可能性是命题 KT45，或 LKT45\[[28](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-28)]。在每种情况下，所讨论的系统都包括一个相关的字母表，通过形式语法构建良构公式，推理（或证明）理论，形式语义学以及至少一些元理论结果（完备性，完全性等）。从标准符号法出发，我们可以这样说，某个特定逻辑系统 LX 中的一组公式ΦLX 可以与某个推理理论一起用于推断某个特定公式ϕLX。（推理可以是演绎的，归纳的，诱导的等等。逻辑主义 AI 在推理方式上没有任何限制。）为了表明这种情况成立，我们写下

ΦLX⊢LXϕLX

当上下文中明确指定了逻辑系统，或者我们不关心涉及的逻辑系统时，我们可以简单地写成

Φ⊢ϕ

每个逻辑系统在其形式语义中都包括用于表示该系统中公式所指向的世界的方式的对象。让这些方式用 WiLX 表示。当我们不关心涉及的逻辑系统时，我们可以简单地写作 Wi。要说这样的方式对公式ϕ进行建模，我们写作

Wi⊨ϕ

我们以自然的方式将其扩展到一组公式：Wi⊨Φ表示Φ中的所有元素在 Wi 上都为真。现在，利用我们已经建立的简单机制，我们可以以宏观的方式描述符合逻辑主义观点的智能体的生活。这种生活符合_AIMA_意义上智能体的基本循环。

首先，我们假设人类设计师在研究世界后，使用特定逻辑系统的语言，给我们的代理人一个关于这个世界的初始信念集合Δ0。在这样做的过程中，设计师使用这个世界的一个形式模型 W，并确保 W⊨Δ0。按照传统，我们将Δ0 称为代理人的（起始）**知识库**。（鉴于我们谈论的是代理人的_信念_，这个术语被认为是奇怪的，但它仍然存在。）接下来，代理人通过调整其知识库来产生一个新的知识库Δ1。我们说调整是通过操作 A 进行的，所以 A\[Δ0]=Δ1。调整过程 A 是如何工作的？有很多可能性。不幸的是，许多人认为最简单的可能性（即 A\[Δi]等于可以从Δi 以某种基本方式推导出的所有公式的集合）耗尽了_所有_可能性。事实是，如上所示，调整可以通过_任何_推理方式进行——归纳、演绎和是的，与所使用的逻辑系统相对应的各种形式的演绎。对于目前的目的，我们不必仔细列举所有的选择。

当代理人试图实现其目标时，循环继续进行，代理人对环境进行**行动**。当然，行动可能会导致环境发生变化。此时，代理人**感知**环境，这个新信息Γ1 会影响到调整的过程，使得 A\[Δ1∪Γ1]=Δ2。**感知⇒调整⇒行动**的循环继续产生我们代理人的生活Δ0,Δ1,Δ2,Δ3,…, …。

这可能让你感到荒谬，逻辑主义人工智能被吹捧为一种复制_所有_认知的方法。在某个逻辑系统中对公式进行推理可能适用于计算捕捉高级任务，比如尝试解决数学问题（或者为《斯坦福哲学百科全书》的条目设计提纲），但是这样的推理如何适用于像鹰在俯冲捕捉匆匆逃逸的猎物时所面临的任务呢？在人类领域，运动员成功完成的任务似乎属于同一类别。肯定会有人宣称，一个外野手追逐飞球并不是通过证明定理来计算如何完成一个扑救来拯救比赛！有两个极端简化的论点可以支持这种“逻辑主义的一切理论”方法来处理认知。第一个论点源于一个事实，即仅仅对一阶逻辑进行完整的证明演算可以模拟图灵级别的计算（第 11 章，Boolos 等人，2007 年）。第二个理由来自逻辑在数学和数学推理的基础理论中的作用。不仅数学的基础理论是用逻辑来构建的（Potter，2004 年），而且已经有成功的项目在机器验证普通非平凡定理方面取得了成果，例如，在[Mizar 项目](http://mizar.org/)中仅验证了大约 50,000 个定理（Naumowicz 和 Kornilowicz，2009 年）。论点是，如果任何人工智能方法都可以用数学方式表达，那么它可以以逻辑主义的形式表达。

不用说，逻辑主义者已经仔细考虑了上述简化论证之外的声明。例如，Rosenschein 和 Kaelbling（1986 年）描述了一种使用逻辑来指定有限状态机的方法。这些机器在“运行时”用于快速反应处理。在这种方法中，虽然有限状态机在传统意义上不包含逻辑，但它们是由逻辑和推理生成的。Amir 和 Maynard-Reid（1999 年，2000 年，2001 年）通过一阶定理证明展示了通过真实机器人控制。实际上，您可以在办公环境中为 Nomad 200 移动机器人下载 2.0 版本的软件，使这种方法成为现实。当然，与洋基队的外野手经常展示的快速调整相比，适应办公环境是相去甚远的，但无疑，未来的机器是否能够通过快速推理来模仿这样的壮举是一个悬而未决的问题。这个问题之所以悬而未决，原因不仅仅是因为所有人都必须承认，一阶定理证明器的推理速度不断增加令人惊叹。（有关此增加的最新消息，请访问并监控[TPTP 网站](http://www.cs.miami.edu/\~tptp)。）目前没有人知道为什么相关的软件工程不能继续产生速度增益，最终使人造生物能够通过纯粹的逻辑主义方式处理信息来接住飞球。

现在我们来到与逻辑主义人工智能相关的第二个话题，值得在此提及的是：通用逻辑和不同逻辑系统之间相互操作性的不断追求。这里只提供一些简短的评论。\[[29](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-29)] 想要了解更多的读者可以在总结过程中探索提供的链接。

一种标准化的方法是通过所谓的通用逻辑（CL）及其变体。（CL 作为一个[ISO 标准](https://www.iso.org/standard/39175.html)发布 - ISO 是国际标准化组织。）对逻辑感兴趣的哲学家和逻辑学家会发现 CL 非常有趣。从历史的角度来看，CL 的出现有趣的一点是，推动它的人正是帕特·海耶斯（Pat Hayes），正是他与麦卡锡一起在 20 世纪 60 年代建立了逻辑主义人工智能。尽管海耶斯没有参加 1956 年的达特茅斯会议，但他无疑应该被视为当代人工智能的创始人之一。至少在我们看来，CL 的一个有趣之处在于它标志着逻辑、编程语言和环境融合的趋势。另一个逻辑/编程混合的系统是[Athena](http://www.proofcentral.org/athena/)，它可以用作编程语言，同时也是一种 MSL 形式。Athena 基于被称为**指示性证明语言**的形式系统（Arkoudas 2000）。

CL 如何实现两个系统之间的互操作性？假设其中一个系统基于逻辑 L，另一个系统基于 L'。（为了简化说明，假设两个逻辑都是一阶逻辑。）思想是，一个理论ΦL，即 L 中的一组公式，可以被翻译成 CL，产生ΦCL，然后这个理论可以被翻译成Φ'L。CL 因此成为一种_inter lingua_。请注意，在 L 中什么被视为良构公式可能与 L'中的不同。这两个逻辑也可能有不同的证明理论。例如，L 中的推理可能基于归结，而 L'中的推理则是自然演绎的。最后，符号集将不同。尽管存在这些差异，但通过翻译，可以在翻译中产生所需的行为。无论如何，这是希望。这里的技术挑战是巨大的，但联邦资金越来越多地用于解决互操作性问题。

现在讨论本节的第三个主题：所谓的**编码降级**。这个技术很容易理解。假设我们手头有一组一阶公理Φ。众所周知，对于任意公式ϕ，判断它是否可以从Φ推导出来是图灵不可判定的：在一般情况下，没有图灵机或等价物可以正确地返回“是”或“否”。然而，如果所讨论的域是有限的，我们可以将这个问题编码降级为命题演算。所有事物都具有 F 的断言当然等价于断言 Fa，Fb，Fc，只要域只包含这三个对象。因此，一阶量化公式在命题演算中变成了一个合取式。确定这样的合取式是否可以从在命题演算中表示的公理中推导出来是图灵可判定的，并且在某些情况下，这个检查可以在命题演算中非常快速地完成；_非常快速_。对于对编码降级到命题演算感兴趣的读者，可以参考 Bart Selman 最近的[DARPA 赞助的工作](http://www.cs.cornell.edu/selman/papers/index.html)。请注意，编码降级的目标不一定是命题演算。因为机器在内涵逻辑中找到证明通常比在直接的一阶逻辑中更困难，所以将前者编码降级为后者通常是方便的。例如，命题模态逻辑可以在多排序逻辑（FOL 的一种变体）中进行编码；参见（Arkoudas＆Bringsjord 2005）。这样的编码降级的重要用途可以在一组称为_描述逻辑_的系统中找到，这些系统比一阶逻辑更具表达能力，但比命题逻辑更具表达能力（Baader 等人，2003）。描述逻辑用于推理给定领域中的本体，并且已经成功地应用于生物医学领域（Smith 等人，2007）。

### 3.3 非逻辑主义人工智能：摘要

诱人的是，可以通过否定来定义非逻辑主义人工智能：一种构建智能代理的方法，拒绝逻辑主义人工智能的特征。这样的捷径会暗示，由非逻辑主义人工智能研究人员和开发人员构建的代理，无论这些代理的优点如何，都不能说知道 ϕ; - 简单地因为，通过否定，非逻辑主义范式甚至没有一个候选的陈述命题是 ϕ;。然而，这并不是一种特别有启发性的定义非符号主义人工智能的方式。更有成效的方法是说，非符号主义人工智能是基于除了逻辑系统之外的特定形式主义进行的人工智能，并列举这些形式主义。当然，结果会发现，这些形式主义在正常意义上不包括知识。（众所周知，在哲学中，正常意义是指如果 p 被知道，那么 p 是一个陈述性陈述。）

从除逻辑系统以外的形式主义的角度来看，非逻辑主义人工智能可以分为符号但非逻辑主义方法和连接主义/神经计算方法。（基于符号的、声明性结构进行的人工智能，这些结构为了可读性和易用性，并不直接被研究人员视为形式逻辑的要素，不计入其中。这类方法包括传统的语义网络、Schank（1972）的概念依赖方案、基于框架的方案和其他类似方案。）前者方法如今是概率性的，并且基于下面所述的形式主义（贝叶斯网络）。后者方法基于我们所指出的可以广义地称为“神经计算”的形式主义。鉴于空间限制，这里只描述这一类别中的一个形式主义（并且简要描述）：前面提到的**人造神经网络**。\[[30](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-30)]。虽然人造神经网络在适当的架构下可以用于任意计算，但它们几乎只用于构建学习系统。

神经网络由用于表示神经元的**单元**或**节点**组成，这些单元通过用于表示树突的**链接**连接在一起，每个链接都有一个数值**权重**。

![通过激活函数生成输出的加权输入之和](https://plato.stanford.edu/entries/artificial-intelligence/neuron-unit\*1.png)

_人造神经网络中的“神经元”（来自 AIMA3e）_

通常假设一些单元与外部环境共生；这些单元形成了**输入**和**输出**单元的集合。每个单元都有一个当前的**激活水平**，即其输出，并且可以根据其输入和这些输入上的权重计算出下一个时刻的激活水平。这个计算完全是局部的：一个单元只考虑网络中的邻居。这个局部计算分为两个阶段。首先，**输入函数**ini 给出了单元输入值的加权和，即输入激活值乘以它们的权重的总和：

ini=∑jWjiaj

在第二阶段，**激活函数** g 接受第一阶段的输入作为参数，并生成输出或激活水平 ai：

ai=g(ini)=g(∑jWjiaj)

一种常见（且公认为基础）的激活函数选择（通常控制给定网络中的所有单元）是阶跃函数，通常具有阈值 t，当输入大于 t 时输出 1，否则输出 0。这被认为在某种程度上类似于大脑，因为 1 代表神经元通过轴突发射脉冲，0 代表没有发射。下图显示了一个简单的三层神经网络。

![具有 3 层的神经网络](https://plato.stanford.edu/entries/artificial-intelligence/neural-net\*1.png)

_一个简单的三层人造神经网络（来自 AIMA3e）_

正如你可以想象的那样，有许多不同类型的神经网络。主要的区别在于**前馈**和**循环**网络。在前馈网络中，如上图所示，正如它们的名字所暗示的那样，链接将信息向一个方向传递，没有循环；而循环网络允许循环回溯，并且可能变得相当复杂。有关更详细的介绍，请参见

[神经网络的补充](https://plato.stanford.edu/entries/artificial-intelligence/neural-nets.html)。

神经网络基本上受到了一个困扰，即虽然它们简单且具有理论上高效的学习算法，但当它们是多层的且足够表达非线性函数时，在实践中很难训练。这在 2000 年代中期发生了变化，因为出现了更好利用先进硬件的方法（Rajat 等人，2009 年）。用于训练多层神经网络的反向传播方法可以转化为对大量数字进行重复简单算术运算的序列。计算硬件的一般趋势是偏向于能够执行大量不太相互依赖的简单操作，而不是少量复杂而复杂的操作。

另一个最近的关键观察是，深度神经网络可以首先在无监督阶段进行预训练，即它们只是被提供数据，而没有任何数据的标签。每个隐藏层被强制表示下一层的输出。这种训练的结果是一系列层，逐渐抽象地表示输入域。例如，如果我们用人脸图像对网络进行预训练，我们将得到一个第一层，它擅长检测图像中的边缘，一个第二层，它可以将边缘组合成眼睛、鼻子等面部特征，一个第三层，它对特征组合做出响应，依此类推（LeCun 等，2015）。

也许在其他统计学习形式和方法的背景下，教授学生有关神经网络的最佳技术是专注于一个特定的问题，最好是一个似乎不自然使用逻辑技术来解决的问题。任务是寻求利用任何可用的技术来设计解决方案。一个很好的问题是手写识别（这也涉及到丰富的哲学维度；参见 Hofstadter 和 McGraw，1995）。例如，考虑这样一个问题：给定一个手写数字 d 作为输入，要求正确地将其分配为 0 到 9 之间的数字。由于研究人员可以获得一个包含 60,000 个标记数字的数据库（来自国家科学技术研究所），这个问题已经成为比较学习算法的基准问题。根据 Benenson（2016）最近的排名，神经网络目前是解决这个问题的最佳方法。

对于对人工智能（和计算认知科学）感兴趣并从明显以大脑为基础的角度追求的读者，鼓励探索 Rick Granger（2004a，2004b）的工作以及他的[Brain Engineering Laboratory](https://www.brainengineering.org/)和[W. H. Neukom Institute for Computational Sciences](https://neukom.dartmouth.edu/)中的研究人员。在原始的 1956 年会议上开始的“干燥”的逻辑主义人工智能与 Granger 和他的合作者采取的直接建模大脑电路的方法之间的对比是显著的。对于那些对神经网络的计算性能感兴趣的人，Hornik 等人（1989）研究了神经网络在学习独立的情况下的一般表示能力。

### 3.4 超越范式冲突的人工智能

在这一点上，读者已经接触到了人工智能中的主要形式主义，并且可能会对将它们联系起来的异质方法产生疑问。在人工智能领域是否存在这样的研究和开发呢？是的。从一个_工程_的角度来看，这样的工作具有不可抗拒的好处。现在人们认识到，为了构建能够完成任务的应用程序，应该从一个工具箱中选择逻辑学、概率/贝叶斯和神经计算技术。鉴于最初的自上而下的逻辑学范式仍然存在并且蓬勃发展（例如，参见 Brachman＆Levesque 2004，Mueller 2006），并且正如前面所提到的，贝叶斯和神经计算方法的复兴也使得这两个范式也站在了坚实而富饶的基础上，人工智能现在以这个基本三元组为武器向前发展，几乎可以肯定的是，应用程序（例如机器人）将通过借鉴这三个范式的元素来进行工程化。Watson 的 DeepQA 架构就是一个利用多种范式的工程系统的最新例子。有关详细讨论，请参见

[Watson 的 DeepQA 架构补充](https://plato.stanford.edu/entries/artificial-intelligence/watson.html)。

Google DeepMind 的 AlphaGo 是多范式系统的另一个例子，尽管形式比 Watson 要窄得多。在诸如围棋或国际象棋等游戏中，中心算法问题是搜索大量有效移动的序列。对于大多数非平凡的游戏来说，这是不可行的。蒙特卡洛树搜索（MCTS）算法通过以统计方式搜索大量有效移动的空间来克服这个障碍（Browne 等，2012 年）。虽然 MCTS 是 AlpaGo 的核心算法，但还有两个神经网络帮助评估游戏中的状态，并帮助模拟专家对手的下棋方式（Silver 等，2016 年）。值得注意的是，MCTS 几乎是通用游戏中所有获胜提交的背后算法（Finnsson，2012 年）。

然而，关于人工智能主要范式的深度理论整合呢？目前，这样的整合只是未来的可能性，但读者可以参考一些致力于这种整合的研究。例如：Sun（1994 年，2002 年）一直致力于证明，表面上是符号性质的人类认知（例如，分析传统中的专业哲学思考，明确处理符号化的论证和定义）可以起源于神经计算性质的认知。Koller（1997 年）研究了概率论和逻辑之间的结合。而且，所谓的“人类水平”人工智能的最新到来，是由寻求真正整合上述三种范式的理论家所引领（例如，Cassimatis，2006 年）。

最后，我们注意到**认知架构**（如 Soar（Laird 2012）和 PolyScheme（Cassimatis 2006））是另一个融合不同 AI 领域的领域。例如，一个致力于构建人级 AI 的努力是 Companions 项目（Forbus 和 Hinrichs 2006）。Companions 是长寿系统，旨在成为与人类合作的人级 AI 系统。Companions 架构试图在一个统一的系统中解决多个 AI 问题，如推理和学习、互动性和长寿性。

## 4. AI 的爆炸性增长

正如我们上面所提到的，过去几十年来，人工智能的研究蓬勃发展。现在我们稍微了解了构成人工智能的内容，我们来快速看一下人工智能的爆炸性增长。

首先，需要澄清一点。我们所说的增长并不是与某个人工智能子领域所提供的资金数量相关的肤浅增长。这种情况在所有领域都经常发生，可以由完全政治和财务变化触发，旨在发展某些领域并减少其他领域。同样，我们所说的增长也不是与围绕人工智能（或其子领域）的工业活动的数量相关的；这种增长也可能是由于与人工智能的科学广度扩展无关的力量驱动的。\[31] 相反，我们所说的是深度内容的爆炸性增长：某人想要熟悉该领域所需了解的新材料。与其他领域相比，这种爆炸的规模可能是前所未有的。（尽管也许应该指出的是，哲学领域的类似增长将以全新的推理形式主义的发展为标志，这反映在长期存在的哲学教材如 Copi 的《逻辑导论》（2004 年）被大幅重写和扩充以包括这些形式主义，而不是仅仅固守不变的核心形式主义，并在多年间通过边缘的渐进改进进行修订。）但它确实看起来非常引人注目，并且值得在这里注意，即使只是因为人工智能的近期发展在很大程度上将围绕着所涉及的新内容是否构成了新的长期研究和开发的基础，否则将无法获得。\[32]

人工智能在各种人造物和应用中的使用也出现了爆炸式增长。虽然我们还远未能构建出具备人类能力或根据上述 Russell/Hutter 定义在所有情景下都能理性行事的机器，但源自人工智能研究的算法现在被广泛应用于各个领域的许多任务中。

### 4.1 机器学习的蓬勃发展

人工智能在应用领域的巨大增长主要得益于在**机器学习**子领域中新算法的发明。机器学习致力于构建系统，在给定任务的理想表现示例或通过重复经验改善任务表现时提高性能。机器学习算法已被应用于语音识别系统、垃圾邮件过滤器、在线欺诈检测系统、产品推荐系统等。目前机器学习的最新技术可以分为三个领域（Murphy 2013，Alpaydin 2014）：

1. **监督学习**：一种学习形式，计算机试图在给定示例（训练数据 T）的情况下学习函数 f 在其定义域中各点的值。

T={⟨x1,f(x1)⟩,⟨x2,f(x2)⟩,…,⟨xn,f(xn)⟩}.

一个示例任务是尝试用一个人的名字标记人脸图像。在监督学习中，监督以函数 f(x)在函数定义域的某个部分中的各个点 x 的值的形式出现。通常以一组固定的输入和输出对的形式给出。设 h 为“学习到的函数”。监督学习的目标是使 h 尽可能地与真实函数 f 在相同的定义域上匹配。**误差**通常以误差函数的形式定义，例如，误差=∑x∈Tδ(f(x)−h(x))，其中 T 是训练数据。还有其他形式的监督和学习目标是可能的。例如，在**主动学习**中，学习算法可以请求任意输入的函数值。监督学习主导着机器学习领域，并且几乎在上述所有实际应用中都有使用。

2. **无监督学习**：在这里，机器试图在给定一些原始数据{x1,x2,…,xn}时找到有用的知识或信息。输入没有与之关联的函数需要学习。这个想法是机器帮助揭示可能隐藏在数据中的有趣模式或信息。无监督学习的一个应用是**数据挖掘**，在其中搜索大量数据以寻找有趣的信息。_PageRank_是 Google 搜索引擎使用的最早的算法之一，可以被视为一个无监督学习系统，它在没有任何人类监督的情况下对页面进行排名（第 14.10 章，Hastie 等人，2009 年）。
3. **强化学习**：在这里，机器被释放到一个环境中，它不断地行动和感知（类似于上面的 Russell/Hutter 观点），只偶尔通过奖励或惩罚的形式接收到对其行为的反馈。机器必须从这个反馈中学会合理地行为。强化学习的一个应用是构建能够玩电脑游戏的代理程序。这里的目标是构建能够将游戏的感知数据在每个时间点映射到一个行动，以帮助赢得游戏或最大化人类玩家对游戏的享受。在大多数游戏中，我们只在游戏结束时或游戏过程中的不频繁时间间隔内知道我们的表现如何（例如，我们觉得自己正在赢的国际象棋游戏可能在最后一刻迅速逆转）。在监督学习中，训练数据具有理想的输入-输出对。这种形式的学习不适用于构建需要在一段时间内运行并且不仅仅根据一个动作而是一系列动作及其对环境的影响来评判的代理程序。强化学习领域通过各种方法来解决这个问题。虽然有点过时，但 Sutton 和 Barto（1998 年）对该领域进行了全面介绍。

除了在传统的 AI 领域中使用之外，机器学习算法还被广泛应用于科学过程的各个阶段。例如，机器学习技术现在常规地应用于分析粒子加速器产生的大量数据。例如，CERN 每秒产生的数据量达到了 1PB（1015 字节），而源自 AI 的统计算法被用于过滤和分析这些数据。粒子加速器在物理学的基础实验研究中用于探索我们物质宇宙的结构。它们通过将较大的粒子碰撞在一起来产生更细小的粒子。并非所有这样的事件都是有意义的。机器学习方法已被用于选择进一步分析的事件（Whiteson 和 Whiteson 2009 以及 Baldi 等人 2014）。最近，CERN 的研究人员发起了一个机器学习竞赛，以帮助分析希格斯玻色子。这个挑战的目标是开发算法，根据 CERN 的大型强子对撞机的数据，将有意义的事件与背景噪声分离开来。

在过去的几十年中，出现了大量没有明确语义的数据。这些数据由人类和机器生成。大部分这些数据不容易被机器处理；例如，图像、文本、视频（与知识库或数据库中精心策划的数据相对）。这导致了一个庞大的行业，应用 AI 技术从这些海量数据中获取可用信息。将从 AI 派生的技术应用于大量数据的领域被称为“数据挖掘”、“大数据”、“分析”等。这个领域太广泛了，本文无法详细介绍，但我们注意到对于什么构成“大数据”问题并没有完全一致的看法。Madden（2012）提出的一个定义是，与传统的机器可处理数据相比，大数据在以下方面有所不同：数据量太大（对于大多数现有的先进硬件来说），速度太快（以快速率生成，例如在线电子邮件交易）或者太难。在太难的部分，AI 技术非常有效。虽然这个领域非常多样化，但我们将在本文后面使用 Watson 系统作为与 AI 相关的示例。正如我们后面将看到的，尽管这个新的爆炸主要由学习驱动，但它并不完全局限于学习。这种学习算法的蓬勃发展得到了神经计算技术和概率技术的支持。

### 4.2 神经计算技术的复兴

《(Charniak & McDermott 1985)》的一个显著特点是：作者们说人工智能的核心教条是“大脑所做的事情可以在某种程度上被视为一种计算”（第 6 页）。然而，在这本书中，几乎没有讨论类似大脑计算的内容。实际上，你会徒劳地在索引中寻找“神经”及其变体的术语。请注意，作者们并不应为此负责。人工智能的很大一部分增长来自于某种程度上基于大脑而非逻辑的形式化、工具和技术。一篇传达神经计算的重要性和成熟性的论文是《(Litt et al. 2006)》。（增长也来自于概率技术的回归，这些技术在 70 年代中期和 80 年代曾经衰退。稍后将在下一个“复兴”[章节](https://plato.stanford.edu/entries/artificial-intelligence/#ResuProbTech)中详细介绍。）

一类非逻辑主义形式主义非常突出，确实向大脑方向致敬：即**人造神经网络**（或者通常简称为**神经网络**，甚至只是**神经网**）。（神经网络的结构和最新发展在[上文](https://plato.stanford.edu/entries/artificial-intelligence/#NonLogiAISumm)中讨论过）。因为明斯基和帕佩特（1969）的《感知机》使得许多人（包括特别是许多 AI 研究和开发的赞助商）得出结论，即神经网络没有足够的信息处理能力来模拟人类认知，这种形式主义在 AI 中几乎被普遍放弃。然而，明斯基和帕佩特只考虑了非常有限的神经网络。**连接主义**认为智能不在于符号处理，而是在于至少在某种程度上类似于大脑（至少在细胞水平上）的非符号处理，特别是通过人造神经网络来近似，这种观点在 20 世纪 80 年代初凭借更复杂的这类网络形式的力量回归，并且很快情况就变成了（用约翰·麦卡锡引入的一个隐喻来说）两匹马在竞赛中建立真正智能代理的竞争。

如果必须选择一个连接主义复兴的年份，那肯定是 1986 年，《并行分布处理》（Rumelhart & McClelland 1986）出版的那一年。连接主义的复兴主要得益于神经网络上的反向传播（backpropagation）算法，这在《AIMA》的第 20 章中有很好的介绍。符号主义者和连接主义者的竞赛在文献中引发了一系列激烈的辩论（例如，Smolensky 1988，Bringsjord 1991），一些 AI 工程师明确支持一种以知识表示和推理的拒绝为特征的方法论。例如，罗德尼·布鲁克斯就是这样的工程师；他写了著名的《无表示的智能》（1991），而他上面提到的 Cog 项目可以说是预谋的非逻辑主义方法的一种体现。然而，越来越多从事构建复杂系统的人发现，需要同时使用逻辑主义和更多的神经计算技术（Wermter & Sun 2001）。此外，如今的神经计算范式将连接主义仅作为其中的一部分，因为一些致力于通过工程化基于大脑的计算的人试图通过神经网络之外的方法来构建智能系统（例如，Granger 2004a，2004b）。

最近，神经计算技术在机器学习领域出现了另一次复兴。机器学习的工作方式是，在给定一个问题（比如识别手写数字{0,1,…,9}或人脸）时，机器学习专家或领域专家会构建一个用于该任务的**特征向量表示**函数。这个函数将输入转换为一种格式，试图丢弃输入中的无关信息，只保留对任务有用的信息。经过转换的输入被称为**特征**。对于识别人脸来说，无关信息可能是场景中的光照量，而相关信息可能是关于面部特征的信息。然后，机器会接收到一系列由特征表示的输入以及这些输入的理想或真实输出值。这将学习挑战从需要从示例中学习函数 f 的挑战转变为需要从可能更容易的数据中学习的挑战。这里的函数 r 是计算输入的特征向量表示的函数。形式上，假设 f 是函数 g 和 r 的组合。也就是说，对于任何输入 x，f(x)=g(r(x))。这用 f=g∘r 表示。对于任何输入，首先计算特征，然后应用函数 g。如果特征表示 r 由领域专家提供，学习问题将变得更简单，特征表示的难度取决于任务的难度。在极端情况下，特征向量可能隐藏在输入中一个容易提取的答案形式，而在另一个极端情况下，特征表示可能只是纯粹的输入。

对于非平凡的问题，选择正确的表示方式至关重要。例如，AI 领域的一个重大变革之一是由于明斯基和帕佩特（1969）证明感知机甚至无法学习二进制的**异或**函数，但如果我们选择正确的表示方式，感知机是可以学习这个函数的。特征工程已经成为机器学习中最费时的任务之一，以至于被认为是机器学习的 _“黑魔法”_ 之一。学习方法的另一个重要的黑魔法是选择正确的参数。这些黑魔法需要丰富的人类专业知识和经验，而这些知识和经验很难在没有充分学徒期的情况下获得（Domingos 2012）。另一个更大的问题是，特征工程的任务只是以新的形式进行的知识表示。

鉴于这种情况，最近出现了一种自动学习特征表示函数 r 的方法的复兴；这些方法有可能绕过传统上需要大量人力的工作。这些方法主要基于现在被称为**深度神经网络**的技术。这些网络只是具有两个或更多隐藏层的神经网络。通过使用一个或多个隐藏层来学习 r，这些网络使我们能够学习特征函数 r。从原始感官数据中学习而不需要太多基于手工特征工程的学习方式现在有了自己的术语：**深度学习**。一个通用而简洁的定义（Bengio 等人，2015 年）是：

> 深度学习可以被安全地视为涉及比传统机器学习更多的学习函数或学习概念组合的模型的研究。（Bengio 等人，2015 年，第 1 章）

虽然这个想法已经存在了几十年，但最近的创新使得更高效的学习技术变得更加可行（Bengio 等人，2013 年）。深度学习方法最近在图像识别（给定包含各种对象的图像，从给定的标签集中标记对象）、语音识别（从音频输入生成文本表示）和粒子加速器数据分析方面取得了最先进的结果（LeCun 等人，2015 年）。尽管在这些任务中取得了令人印象深刻的结果，但仍存在一些未解决的次要和重大问题。一个次要问题是仍然需要重要的人类专业知识来选择架构并设置正确的参数；一个重大问题是所谓的**对抗输入**的存在，这些输入在人类看来与正常输入无法区分，但是通过特殊的计算方式使得神经网络将它们视为与训练数据中的类似输入不同。这种对抗输入的存在在训练数据中保持稳定，引发了对基准测试结果如何转化为具有感知噪声的真实系统性能的质疑（Szegedy 等人，2014 年）。

### 4.3 概率技术的复兴

人工智能爆炸性增长的第二个维度是概率方法的爆炸性流行，这些方法并非神经计算的本质，而是为了在面对不确定性时形式化和机械化一种非逻辑主义的推理形式。有趣的是，尤金·查尼亚克本人可以被安全地认为是明确、预谋地从逻辑转向统计技术的主要倡导者之一。他的专业领域是自然语言处理，而他在 1985 年的入门教材准确地展示了他当时的解析方法（正如我们所见，编写计算机程序，以英文文本作为输入，最终推断出用 FOL 表达的含义），但这种方法被纯粹的统计方法所取代（查尼亚克，1993 年）。在[AI@50](https://web.archive.org/web/20200315115548/http://www.dartmouth.edu/\~ai50/homepage.html)会议上，查尼亚克大胆宣称，在一个寓意深长的演讲中，题为“为什么自然语言处理现在是统计自然语言处理”，逻辑主义人工智能已经停滞不前，而统计方法是唯一有前途的游戏-在接下来的 50 年里。\[34]

会议上的主要能源和辩论源于查尼亚克的概率取向与会议上的约翰·麦卡锡和其他人所坚持的原始逻辑主义取向之间的冲突。

AI 对概率论的应用源于该理论的标准形式，该形式直接源于技术哲学和逻辑学。这种形式对许多哲学家来说是熟悉的，但现在让我们快速回顾一下，以便为讨论新的概率技术奠定坚实的基础。

就像在 FOL 的情况下一样，在概率论中，我们关注的是陈述性语句，或称为**命题**，对其应用信念程度；因此我们可以说逻辑主义和概率主义方法都是符号性的。这两种方法也都认同陈述可以在世界中是真或假的。在构建代理程序时，简单的基于逻辑的方法要求代理程序知道所有可能陈述的真值。这是不现实的，因为代理程序可能由于无知、物理世界的非确定性或陈述的含义模糊而不知道某个命题 p 的真值。更具体地说，概率论中的基本命题是一个**随机变量**，可以被看作是代理程序最初不知道的世界的一个方面。我们通常将随机变量的名称大写，尽管我们也将 p、q、r 等作为这样的名称保留。例如，在一个关于 Barolo 先生是否犯罪的特定谋杀调查中，随机变量 Guilty 可能是关注的焦点。侦探也可能对谋杀武器（假设是一把特定的刀）是否属于 Barolo 感兴趣。基于此，我们可以说如果属于，那么 Weapon=true，如果不属于，那么 Weapon=false。作为一种符号上的便利，我们可以分别用 weapon 和¬weapon 来表示这两种情况；我们可以对这类变量使用这种约定。

到目前为止，我们描述的变量类型是布尔型，因为它们的域只是{true,false}。但我们可以推广并允许离散的随机变量，其值可以来自任何可数的域。例如，PriceTChina 可能是中国茶叶的价格变量（一个特定的茶叶），其域可能是{1,2,3,4,5}，其中每个数字表示美元。第三种类型的变量是连续的，其域可以是实数或其子集。

我们称**原子事件**是将适当域中的特定值分配给构成（理想化的）世界的所有变量。例如，在刚才介绍的简单谋杀调查世界中，我们有两个布尔变量 Guilty 和 Weapon，只有四个原子事件。注意，原子事件具有一些明显的特性。例如，它们是互斥的、穷尽的，并且逻辑上蕴含每个命题的真实性或虚假性。对于初学者来说，通常不明显的是第四个特性，即任何命题在逻辑上等价于蕴含该命题的所有原子事件的析取。

先验概率对应于在完全没有其他信息的情况下对一个命题的信念程度。例如，如果巴罗洛有罪的先验概率为 0.2，我们写作

P(有罪=true)=0.2

或者简单地说，P(有罪)=0.2。通常方便的是有一种符号表示方法，可以经济地引用随机变量的所有可能值的概率。例如，我们可以写成

P(中国价格)

作为列出中国茶可能价格的五个方程的缩写。我们也可以写成

P(中国茶价格)=⟨1,2,3,4,5⟩

此外，为了进一步方便表示，我们可以写成 P(有罪,武器) 来表示相关随机变量的所有值组合的概率。这被称为 **联合概率分布** 的 有罪 和 武器。 **完整的** 联合概率分布涵盖了用于描述一个世界的所有随机变量的分布。鉴于我们简单的谋杀世界，我们有 20 个原子事件总结在以下方程中：

P(有罪,武器,中国价格)

概率论基本语言的最后一部分对应于**条件**概率。其中 p 和 q 是任意命题，相关表达式是 P(p∣q)，可以解释为“在我们所知道的只有 q 的情况下，p 的概率”。例如，

P(有罪∣∣武器)=0.7

说如果凶器属于巴罗洛，并且没有其他信息可用，那么巴罗洛有罪的概率为 0.7。

安德烈·科尔莫哥洛夫展示了如何从现在引入的机制中构建概率论的三个公理。

1. 所有的概率都介于 0 和 1 之间。即，∀p.0≤P(p)≤1。
2. 在传统逻辑学意义上，有效的命题概率为 1；不可满足的命题概率为 0。
3. P(p∨q)=P(p)+P(q)−P(p∧q)

这些公理显然是基于逻辑主义的。概率论的其余部分可以建立在这个基础上（条件概率可以很容易地用先验概率来定义）。因此，我们可以说，在某种根本意义上，逻辑仍然被用来描述一个理性主体可能拥有的信念集合。但是，在这种观点中，概率推理又是如何进入图景的呢？因为传统的演绎推理并不用于概率论中的推理？

概率推理是根据概率理论表达的观察证据，计算感兴趣命题的后验概率。很长一段时间以来，已经有算法来进行这种计算。这些算法先于概率技术在 1990 年代的复兴。（《AIMA》的第 13 章介绍了其中一些算法。）例如，根据科尔莫哥洛夫公理，可以通过使用给出所有原子事件概率的完全联合分布的直接方法来计算任何命题的概率：设 p 是某个命题，让α(p)是所有包含 p 的原子事件的析取。由于命题的概率（即 P(p)）等于它包含的原子事件的概率之和，我们有一个方程提供了计算任何命题 p 概率的方法，即

P(p)=∑ei∈α(p)P(ei)

不幸的是，这种原始的概率方法存在两个严重的问题：一是所需处理的信息量巨大（需要对整个分布进行枚举），令人不知所措。二是这种方法的表达能力仅限于命题层面。（顺便提一下，哲学家希拉里·普特南（1963）指出，转向一阶层面是有代价的。这个问题在此不讨论。）一切都随着一种将概率论和图论结合的新形式主义的出现而改变：**贝叶斯网络**（也称为**信念网络**）。关键的文献是（Pearl 1988）。有关更详细的讨论，请参阅

[贝叶斯网络补充](https://plato.stanford.edu/entries/artificial-intelligence/bayesian-nets.html)。

在结束本节之前，值得注意的是，从哲学的角度来看，像我们上面利用的谋杀调查这样的情况通常会被分析为_论证_和强度因素，而不是纯粹的算术程序来计算的数字。例如，在罗德里克·奇斯霍姆的认识论中，正如他在《知识论》（1966 年，1977 年）中所提出的，侦探福尔摩斯可能会将像_巴罗洛犯了谋杀_这样的命题分类为**平衡**，如果他无法找到一个令人信服的论证，或者如果谋杀武器被证明属于巴罗洛，那么可能是**可能的**。这些类别无法在 0 到 1 的连续范围内找到，并且它们用于表达对巴罗洛有罪或无罪的论证。基于论证的不确定和可推翻推理方法在人工智能中几乎不存在。波洛克的方法是一个例外，下面将对其进行介绍。这种方法与奇斯霍姆的性质相似。

还应该注意到，已经有了处理概率推理的成熟形式化方法，将其视为基于逻辑的推理的一个实例。例如，当概率推理研究人员证明关于他们领域的定理ϕ（例如（Pearl 1988）中的任何定理）时，他们所进行的活动纯粹属于传统逻辑的范畴。对于对概率推理具有逻辑风格的方法感兴趣的读者可以参考（Adams 1996，Hailperin 1996 和 2010，Halpern 1998）。将概率论、归纳推理和演绎推理结合起来，使它们处于平等地位的形式化方法已经越来越多，其中马尔可夫逻辑（Richardson and Domingos 2006）是其中突出的方法之一。

**概率机器学习**

机器学习在上述意义上与概率技术相关联。概率技术与函数学习（例如朴素贝叶斯分类）和学习算法的理论属性建模相关。例如，监督学习的标准重新表述将其视为**贝叶斯问题**。假设我们要识别给定图像中的数字\[0-9]。解决这个问题的一种方法是询问在传感器给出的图像 d 的情况下，假设 Hx：“数字是 x”的概率是多少。贝叶斯定理给出了：

P(Hx∣∣d)=P(d∣∣Hx)∗P(Hx)P(d)

可以从给定的训练数据集中估计出 P(d∣Hx)和 P(Hx)。然后，具有最高后验概率的假设被给定为答案，由 argmaxxP(d∣∣Hx)∗P(Hx)给出。除了使用概率方法构建算法外，概率论还被用于分析可能没有明显概率或逻辑公式的算法。例如，在学习中的一个中心类别的元定理，即**可能近似正确（PAC）** 定理，是以概率下界的形式表达的，该概率表示引导/学习的**fL**函数与真实函数**fT**之间的不匹配程度小于某个特定值，前提是学习的函数**fL**在某些情况下表现良好（见第 18 章，AIMA）。

## 5. 人造物在野外

从至少现代起，人造物一直与设备相连，通常是由公司生产的设备，如果我们不谈论这一现象，那将是不妥的。虽然在人造物及其相关领域（如优化和决策）中有许多商业上的成功案例，但有些应用更加显眼，并且在野外经过了充分的实战测试。2014 年，最显眼的一个领域（人造物在其中取得了显著的成功）是信息检索，以网络搜索的形式呈现。另一个最近的成功案例是模式识别。应用模式识别的最新技术（如指纹/面部验证、语音识别和手写识别）已经足够强大，可以在实验室之外进行“高风险”部署。截至 2018 年中，一些公司和研究实验室已经开始在公共道路上测试自动驾驶车辆，甚至有少数司法管辖区将自动驾驶汽车合法化。例如，谷歌的自动驾驶汽车在加利福尼亚州在非常复杂的条件下，几乎没有人类的帮助下行驶了数十万英里（Guizzo 2011）。

计算机游戏为人工智能技术提供了一个强大的测试平台，因为它们可以捕捉到测试人工智能技术所必需的重要部分，同时抽象或移除可能超出核心人工智能研究范围的细节，例如设计更好的硬件或处理法律问题（Laird 和 VanLent 2001）。在商业部署人工智能方面取得了相当成功的游戏子类是实时战略游戏。实时战略游戏是玩家在有限资源下管理军队的游戏。一个目标是不断与其他玩家战斗并削弱对手的力量。实时战略游戏与策略游戏的不同之处在于玩家实时同时规划行动，而不需要轮流进行。这类游戏具有一些令人着迷的挑战，这使得这类游戏成为部署简单人工智能代理的有吸引力的场所。关于实时战略游戏中使用的人工智能的概述可以在（Robertson 和 Watson 2015）中找到。

尽管取得了显著的成功，人工智能的一些其他尝试却在默默地缓慢进行。例如，与数十年来一直没有解决方案的数学开放问题相比，与人工智能相关的方法在解决这些问题上取得了胜利。其中最值得注意的问题可能是证明“_所有的 Robbins 代数都是布尔代数_”的陈述。这个问题在上世纪 30 年代被猜测出来，证明最终在 Otter 自动定理证明器的努力下于 1996 年发现，仅经过几个月的努力（Kolata 1996，Wos 2013）。类似的领域，如形式验证，也已经发展到了可以半自动验证重要的硬件/软件组件的程度（Kaufmann 等人 2000 和 Chajed 等人 2017）。

其他相关领域，如（自然）语言翻译，仍有很长的路要走，但已经足够好，让我们在受限条件下使用它们。对于机器翻译等任务，目前尚无定论，似乎需要同时使用统计方法（Lopez 2008）和符号方法（España-Bonet 2011）。这两种方法现在在实际应用中取得了可比较但有限的成功。福特公司部署的翻译系统最初是为将制造过程指令从英语翻译成其他语言而开发的，最初是基于规则的系统，具有福特公司和领域特定的词汇和语言。随着该系统在翻译手册之外获得新的用途，例如福特公司内部的普通用户翻译自己的文件（Rychtyckyj and Plesco 2012），它逐渐发展为结合统计技术和基于规则的技术的系统。

迄今为止，人工智能在上述众多重要领域中取得的伟大成就都是在有限的、狭窄的领域内。在无限制的一般情况下，缺乏任何成功导致一小部分研究人员分离出来，形成了现在所称的“人造通用智能”（Goertzel and Pennachin 2007）。这一运动的声明目标包括再次将重点转向构建能够在各个领域都具备智能而不仅仅是在一个狭窄领域有能力的人造物。

## 6. 道德人工智能

_计算机伦理学_已经存在很长时间了。在这个子领域中，通常会考虑在涉及计算机技术的某类情况下，人们应该如何行动，这里的“人们”指的是人类（Moor 1985）。所谓的“机器人伦理学”是不同的。在这个子领域中（也被称为“道德人工智能”，“伦理人工智能”，“机器伦理学”，“道德机器人”等等），人们面临的问题是机器人能够做出自主且重要的决策，这些决策可能是道德上允许的，也可能不是（Wallach＆Allen 2010）。如果有人试图设计一个具有复杂道德推理和决策能力的机器人，那么他也在进行哲学人工智能的工作，因为这个概念在本文中的其他地方有所描述。道德人工智能可以有许多不同的方法。Wallach 和 Allen（2010）对不同方法进行了高层次的概述。显然，在具有致命行动能力的机器人中需要道德推理。Arkin（2009）介绍了如何控制和规范具有致命行为能力的机器。道德人工智能超越了显然致命的情况，我们可以拥有一系列道德机器。Moor（2006）提供了一种可能的道德代理人的谱系。非致命但具有伦理问题的机器的一个例子是撒谎机器。Clark（2010）使用**心灵的计算理论**，即代表和推理其他代理的能力，来构建一个成功说服人们相信谬误的撒谎机器。Bello＆Bringsjord（2013）概述了构建道德机器可能需要的一般概念，其中之一是心灵理论。

构建能够进行伦理推理的机器的最一般框架是赋予机器一种**道德准则**。这要求机器用于推理的形式框架足够表达这些准则。目前，道德人工智能领域并不关注这些准则的来源或出处。准则的来源可以是人类，机器可以直接（通过明确编码）或间接（阅读）接收到这些准则。另一种可能性是机器从更基本的法则集中推断出这些准则。我们假设机器可以访问某种准则，然后尝试设计机器以确保在所有情况下遵循该准则，同时确保道德准则及其表达不会导致意外后果。**义务逻辑**是一类为此目的而研究最多的形式逻辑。抽象地说，这些逻辑主要关注给定道德准则的推论。然后工程学研究给定义务逻辑与道德准则的匹配程度（即逻辑是否足够表达），这必须与自动化的便利性相平衡。Bringsjord 等人（2006）提供了使用义务逻辑构建能够按照道德准则执行行动的系统的蓝图。义务逻辑在 Bringsjord 等人提供的框架中的作用（可以被认为是道德人工智能领域的代表）最好理解为追求莱布尼茨对于普遍道德计算的梦想：

> 当争议出现时，两位哲学家之间就不再需要辩论，就像两位会计师之间也不需要辩论一样。他们只需拿起笔，在算盘前坐下，然后彼此说（或许召集一个共同的朋友）：“让我们计算一下。”

基于义务逻辑的框架也可以类比于道德自我反思的方式来使用。在这种模式下，机器人在进入现实世界之前可以通过基于逻辑的验证来验证其内部模块。Govindarajulu 和 Bringsjord（2015）提出了一种方法，借鉴了形式化程序验证的思想，其中基于义务逻辑的系统可以用来验证机器人在特定条件下以某种道德合规的方式行动。由于形式验证方法可以用来断言关于无限数量的情况和条件的陈述，因此这种方法可能更受青睐，而不是让机器人在一个道德充满的测试环境中漫游，并做出一系列有限的决策，然后对其道德正确性进行评判。最近，Govindarajulu 和 Bringsjord（2017）使用义务逻辑提出了一个计算模型，用于解释[双重效应原则](https://plato.stanford.edu/entries/double-effect/)，这是一种用于道德困境的伦理原则，已经被哲学家们进行了实证研究和广泛分析。\[[35](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-35)]该原则通常通过使用有轨电车的困境来进行演示和解释，最早由 Foot（1967）以这种方式提出。

虽然在理论和哲学方面已经有了大量的工作，但机器伦理领域仍处于起步阶段。在构建道德机器方面已经有了一些雏形性的工作。最近的一个例子是 Pereira 和 Saptawijaya（2016），他们使用逻辑编程，并以 Scanlon（1982）提出的伦理理论“契约主义”为基础来进行机器伦理的工作。那么未来呢？由于人工智能代理必然会变得越来越聪明，拥有越来越多的自主性和责任性，机器人伦理几乎肯定会变得越来越重要。这个努力可能不是对经典伦理学的直接应用。例如，实验结果表明，人们对机器人的道德标准与他们对类似条件下的人类的期望不同（Malle 等人，2015）。\[[36](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-36)]

## 7. 哲学人工智能

请注意，本节的标题不是关于人工智能的哲学。我们稍后会讨论到那个类别。（目前可以将其视为试图回答以下问题的尝试：人工智能中创建的人造智能是否能够达到人类智能的完全高度。）哲学人工智能是人工智能，而不是哲学；但它是根植于哲学并从哲学中流淌出来的人工智能。例如，一个人可以使用哲学的工具和技术来探讨一个悖论，找出一个提议的解决方案，然后继续进行一步，这对哲学家来说肯定是可选的：用可以转化为计算机程序的术语表达解决方案，当执行时，允许人造智能克服原始悖论的具体实例。\[[37](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-37)] 在我们通过一个特定的研究计划来明确表述这种类型的哲学人工智能之前，让我们首先考虑一下 AI 实际上是否只是哲学，或者是哲学的一部分。

丹尼尔·丹尼特（1979 年）曾声称，人工智能不仅与哲学密切相关，而且人工智能_就是_哲学（至少是认知心理学）。（他对人造生命也提出了类似的观点（丹尼特 1998 年））。这种观点最终被证明是错误的，但它的错误之处将会启发我们，我们的讨论将为哲学人工智能的讨论铺平道路。

那么，丹尼特到底说了什么呢？就是这个：

我想要主张，人工智能最好被视为与传统认识论共享一种最一般、最抽象的自上而下的问题提问方式的地位：知识是如何可能的？（Dennett 1979, 60）

在其他地方，他说他的观点是，人工智能应该被视为“对智能或知识可能性的最抽象的探究”（Dennett 1979, 64）。

简而言之，丹尼特认为人工智能是试图通过设计和实现抽象算法来解释智能，而不是通过研究大脑以期望找到可以将认知分解为组成部分的方法，也不是通过构建底层信息处理单元来逐步建立高级认知过程的方法。他称这种方法是“自上而下”的。抛开至少从 20 世纪 80 年代初开始，人工智能包括一种在某种意义上是自下而上的方法（参见上文中讨论的神经计算范式，以及具体的反例格兰杰（Granger）的工作），丹尼特的观点存在一个致命的缺陷。丹尼特意识到了这个潜在的缺陷，正如他在下面引用的话中所反映的：

> 一些哲学家认为，人工智能不能合理地被这样解释，因为它承担了额外的负担：它限制自己只能提供“机械化”的解决方案，因此它的领域不是康德式的所有可能智能模式的领域，而只是所有可能在机械上实现的智能模式的领域。据称，这将对生命论者、二元论者和其他反机械主义者提出质疑。（丹尼特，1979 年，61 页）

Dennett 对这个反对意见有一个准备好的回答。他写道：

> 但是……人工智能的机制要求并不是任何时刻的额外限制，因为如果心理学有可能存在，并且如果丘奇的论题是真实的，那么机制的限制并不比心理学中的迎合问题的限制更严格，而谁又希望逃避这一点呢？（Dennett 1979, 61）

不幸的是，这是一个非常棘手的问题；对问题的审视揭示了人工智能的本质。

首先，就哲学和心理学关于心灵本质的问题而言，它们并不受到思维是计算的前提的束缚。人工智能，至少是“强人工智能”（我们将在下文讨论“强人工智能”与“弱人工智能”）的一种尝试，通过工程设计某些令人印象深刻的人造物，来证明智能在本质上是计算的（在图灵机及其等价物，例如寄存器机的层面上）。因此，这确实是一个哲学上的主张。但这并不意味着人工智能就是哲学，就像一些物理学家的一些更深入、更激进的主张（例如，宇宙在本质上是数字化的）并不意味着物理学就是哲学。物理学的哲学当然会考虑到这样一个命题，即物理宇宙可以以数字化的方式完美建模（例如一系列的元胞自动机），但当然物理学的哲学不能被等同于这个教义。

其次，我们现在很清楚（那些熟悉相关形式领域的人在丹尼特写作时就已经知道）信息处理可以超越标准计算，也就是说，可以超越图灵机能够完成的计算（我们将称之为图灵计算）。（这种信息处理被称为超计算，这是哲学家杰克·科普兰德创造的一个术语，他本人已经定义了这样的机器（例如，科普兰德 1998 年）。第一台能够进行超计算的机器是“试错机器”，它们是在同一期著名的《符号逻辑杂志》上介绍的（戈尔德 1965 年；普特南 1965 年）。一种新的超计算机是无限时间图灵机（Hamkins＆Lewis 2000）。）丹尼特对教堂论的引用与数学事实相悖：某些类型的信息处理超越了标准计算（或图灵计算）。教堂论，或更准确地说，教堂-图灵论，认为如果一个函数 f 是有效可计算的，那么当且仅当 f 是图灵可计算的（即，某个图灵机可以计算 f）。因此，这个论题对于超越图灵机所能实现的信息处理无能为力。（换句话说，在能够超越图灵机的信息处理设备中，没有反例可以自动找到 CTT。）就哲学和心理学所知，智能即使与信息处理相关，也超过了图灵计算或图灵机械。\[38]这一点尤其正确，因为哲学和心理学与人工智能不同，它们在根本上不负责工程制造人造物，这使得超计算的物理可实现性对它们来说是无关紧要的。因此，与丹尼特相反，将人工智能视为心理学或哲学是犯了一个严重的错误，因为这样做将把这些领域局限于从自然数（包括其中的元组）到自然数的函数空间中的一小部分。（在这个空间中，只有一小部分函数是图灵可计算的。）人工智能毫无疑问比这两个领域狭窄得多。当然，人工智能可能被一个致力于通过编写计算机程序并在具体图灵机上运行它们来构建计算人造物的领域所取代。但是，根据定义，这个新领域将不再是人工智能。我们对 AIMA 和其他教科书的探索直接证实了这一点。

第三，事实上，大多数人工智能研究人员和开发人员只关心构建有用的、有利可图的人造物，并不花太多时间思考本文中探讨的智能的抽象定义（例如，什么是人工智能？）。

虽然人工智能不是哲学，但确实有一些与哲学密切相关的高水平实施导向的人工智能方法。最好的方法是简单地展示这样的研究和开发，或者至少提供一个代表性的例子。虽然已经有许多这样的工作例子，但在人工智能领域最著名的例子是约翰·波洛克的 OSCAR 项目，该项目贯穿了他的大部分生命。有关详细介绍和进一步讨论，请参阅[OSCAR 项目的补充材料](https://plato.stanford.edu/entries/artificial-intelligence/oscar.html)。

值得注意的是，在这个时刻，OSCAR 项目及其所依赖的信息处理无疑是哲学和技术人工智能的结合。鉴于相关工作已经出现在《人工智能》这一致力于该领域的一流期刊上，而不是哲学期刊，这是不可否认的（参见，例如，Pollock 2001, 1992）。这一点很重要，因为虽然在当前场合强调人工智能与哲学之间的联系是合适的，但一些读者可能会怀疑这种强调是否是刻意的：他们可能怀疑事实是 AI 期刊的一页页都充斥着与哲学相距甚远的狭隘技术内容。确实存在许多这样的论文。但我们必须区分旨在展示 AI 的性质、核心方法和目标的著作，与旨在展示特定技术问题进展的著作。

后一类别的著作往往非常狭窄，但正如 Pollock 的例子所示，有时这些特定问题与哲学密不可分。当然，Pollock 的工作是一个典型的例子（尽管是最实质性的一个）。我们也可以选择那些不仅仅从事纯粹哲学研究的人的作品。例如，对于一本完全在人工智能和计算机科学范围内撰写的书，但在很多方面都是认识逻辑的实践，适合在相关研讨会上使用，可以参考（Fagin et al. 2004）。（很难找到与哲学没有直接联系的技术性工作。例如，关于学习的 AI 研究与归纳的哲学处理密切相关，即如何学习真正新的概念，而不仅仅是基于先前的概念进行定义。AI 提供的一个可能的部分答案是**归纳逻辑编程**，详见《AIMA》第 19 章。）

前一类的写作如何呢？这类写作虽然在 AI 领域，而非哲学领域，但仍然具有哲学性质。大多数教科书都包含了大量属于后一类的材料，因此它们包括了对 AI 的哲学性质的讨论（例如，AI 的目标是构建人造智能，这也是为什么它被称为“AI”的原因）。

### 8.1 "强人工智能"与"弱人工智能"

回想一下，我们之前讨论过关于人工智能的定义，并且特别要记住这些定义是以该领域的_目标_为基础的。我们可以在这里遵循这个模式：通过注意到这两种人工智能版本所追求的不同目标，我们可以区分"强人工智能"和"弱人工智能"。"强人工智能"旨在创造出拥有我们所有心智能力的人造人：包括有感知意识的机器。另一方面，"弱人工智能"旨在构建信息处理机器，这些机器_看起来_具备人类的全部心智能力（Searle 1997）。"弱人工智能"也可以被定义为旨在通过不仅仅通过图灵测试（再次简称为 TT），而是通过_全面_图灵测试（Harnad 1991）的形式的人工智能。在全面图灵测试中，机器必须不仅仅在语言上无法区分，还必须在所有行为上都能被认为是人类 - 比如投掷棒球、进食、教课等。

对于哲学家来说，要推翻“弱”人工智能（Bringsjord 和 Xiao 2000）似乎确实非常困难。毕竟，有什么_哲学_原因会阻止人工智能制造出看起来像动物甚至人类的人造物呢？然而，一些哲学家致力于推翻“强”人工智能，我们现在转向最突出的例子。

### 8.2 中国屋论证反对“强人工智能”

毫无疑问，人工智能哲学中最著名的论证是约翰·西尔的（1980 年）中文房间论证（CRA），旨在推翻“强人工智能”。我们在这里简要概述，并从人工智能从业者的角度提供一份“战地报告”来评估这个论证。希望进一步研究中文房间论证的读者可以在[中文房间论证](https://plato.stanford.edu/entries/chinese-room/)和（Bishop＆Preston 2002）的条目中找到一个很好的下一步。

CRA 基于一个思想实验，其中西尔本人扮演主角。他在一个房间里，房间外是不知道西尔在里面的中国本土人。像现实生活中的西尔一样，房间里的西尔不懂中文，但懂得英文。中国人将问题写在卡片上通过一个槽送入房间。通过西尔在房间里的秘密工作，盒子会将卡片作为输出返回给中国本土人。西尔的输出是通过查阅一本规则书来产生的：这本书是一本查找表，告诉他根据输入产生什么中文。对于西尔来说，中文只是一堆-用西尔的话说-蠕动符号。下面的示意图总结了这种情况。标签应该很明显。O 表示外部观察者，即中国人。输入用 i 表示，输出用 o 表示。如您所见，有一个规则书的图标，西尔本人用 P 表示。

![中文房间的输入/输出图](https://plato.stanford.edu/entries/artificial-intelligence/cr1.png)

_中文房间，示意图_

现在，基于这个思想实验的论点是什么？即使你以前从未听说过 CRA，你无疑可以看出基本思想：塞尔（在盒子里）被认为是计算机所能拥有的一切，因为他不懂中文，所以没有计算机能够具有这样的理解能力。塞尔只是毫无意识地移动着波浪线，而（根据这个论点）这就是计算机的全部本质。\[[39](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-39)]

CRA 在今天的地位如何？正如我们已经提到的，这个论点似乎仍然活跃且有力，可以看到（Bishop＆Preston 2002）。然而，毫无疑问，至少在人工智能_从业者_中，CRA 通常被拒绝。（这当然是毫不奇怪的。）在这些从业者中，AI 本身提出了最有力的回应的哲学家是 Rapaport（1988），他认为虽然 AI 系统确实是句法的，但正确的句法可以构成语义。应该说，“强大”AI 的支持者普遍认为 CRA 不仅不合理，而且愚蠢，因为它基于一个离 AI 实践相去甚远的幻想故事（CR），而 AI 实践正逐年不可避免地朝着将彻底消除 CRA 及其支持者的复杂机器人发展。例如，约翰·波洛克（正如我们所指出的，哲学家和 AI 从业者）写道：

一旦\[我的智能系统]OSCAR 完全运作，类比论将不可避免地使我们将思想和感情归因于 OSCAR，其凭证与我们将其归因于人类的凭证完全相同。相反的哲学论证将过时。(Pollock 1995, p. 6)

在总结对 CRA 的讨论时，我们提出两个要点，即：

1. 尽管像波洛克这样的人对于人工智能系统 OSCAR（以及其他许多不断改进的人工智能系统）最终达到人类水平的能力而使 CRA 变得无关紧要充满信心，但残酷的事实是，深度语义自然语言处理（NLP）在当今很少被追求，因此 CRA 的支持者在当前人工智能的现状下肯定不会感到不安。简而言之，西尔将正确地指出人工智能的任何成功案例，包括我们讨论过的沃森系统，仍然宣称理解无处可寻 - 他在哲学上有权这样说。
2. 根据经验事实，似乎 CRA 正在重新引起关注，这是多年来所未见的，因为某些思想家现在明确发出警告，即未来有意识的恶意机器可能会对我们的物种造成威胁。作为回应，西尔（2014）指出，由于 CRA 是正确的，所以不能有有意识的机器；如果没有有意识的机器，就不可能有任何愿望的恶意机器。我们在本文末尾会再次讨论这个问题；这里的主要观点是 CRA 仍然非常相关，我们确实怀疑西尔的无畏基础将被不仅是哲学家，还有人工智能专家、未来学家、律师和政策制定者积极采纳。

读者可能会想知道，在从事人工智能研究的过程中（与他们可能参加哲学会议时相反），是否存在他们参与的哲学辩论。当然，人工智能研究人员之间会进行哲学讨论，对吗？

一般来说，人工智能研究人员确实会在他们之间讨论人工智能哲学的话题，而这些话题通常也是占据着人工智能哲学家们的关注。然而，波洛克上面引用的态度绝对是占主导地位的。也就是说，总体上，人工智能研究人员的态度是，哲学思辨有时很有趣，但人工智能工程的不断进步是不可阻挡的，不会失败，并最终使这种哲学思辨变得无用。

我们将在本条目的[最后一节](https://plato.stanford.edu/entries/artificial-intelligence/#Futu)中回到人工智能的未来问题。

### 8.3 哥德尔论证反对“强人工智能”

四十年前，J.R. Lucas（1964）认为哥德尔的第一不完备定理意味着没有机器能够达到人类水平的智能。他的论点并没有被证明是令人信服的，但是 Lucas 引发了一场争论，产生了更有力的论据。Lucas 的坚定捍卫者之一是物理学家罗杰·彭罗斯（Roger Penrose），他首次试图为 Lucas 辩护的是在他的《皇帝的新脑》（1989）中表达的对“强人工智能”的哥德尔攻击。这个第一次尝试不够完善，彭罗斯发表了一个更详细、更谨慎的哥德尔案例，表达在他的《心灵的阴影》（1994）的第 2 章和第 3 章。

鉴于读者可以参考《哥德尔的不完备性定理》的条目，这里不需要进行全面的回顾。相反，读者可以通过阅读一篇在线论文来对这个论点有一个良好的理解，彭罗斯在这篇论文中回应了他的《心灵的阴影》的批评者（例如哲学家大卫·查尔默斯、逻辑学家所罗门·费弗曼和计算机科学家德鲁·麦克德莫特），将论点浓缩成了几段话。事实上，在这篇论文中，彭罗斯给出了他认为是《心灵的阴影》中核心哥德尔案例的完善版本。以下是这个版本的原文：

我们尝试假设，原则上人类可以访问的（无可辩驳的）数学推理方法的总体可以被封装在某个（不一定是计算机的）健全形式系统 F 中。如果一个人类数学家被呈现了 F，他可以如下论证（请记住，“我是 F”这个短语仅仅是“F 封装了所有人类可访问的数学证明方法”的简写）：

（A）“虽然我不确定我是否是 F，但我得出结论，如果我是 F，那么系统 F 必须是完备的，更重要的是，F'也必须是完备的，其中 F'是通过进一步断言“我是 F”来补充 F。我认识到，假设我是 F，那么哥德尔陈述 G(F')必须是真的，并且它不是 F'的推论。但我刚刚认识到“如果我碰巧是 F，那么 G(F')必须是真的”，而这种认识正是 F'所要实现的。因此，我能够认识到 F'的能力之外的东西，我推断我毕竟不可能是 F。此外，这适用于任何其他（哥德尔化的）系统，而不仅仅是 F。”（Penrose 1996, 3.2）

这个论证是否成功？在本条目中，寻求一个确定的答案是不合适的。鼓励感兴趣的读者参考四个全面的论述（LaForte et. al 1998; Bringsjord and Xiao 2000; Shapiro 2003; Bowie 1982）。

### 8.4 哲学人工智能的其他主题和阅读材料

除了上面简要介绍的哥德尔和西尔的论证之外，还有第三种对“强人工智能”（符号类型）的攻击被广泛讨论（尽管随着统计机器学习的兴起，对它的关注逐渐减少），这种攻击由哲学家休伯特·德雷福斯（Hubert Dreyfus）提出（1972 年，1992 年），其中的某些表达方式与他的兄弟斯图尔特·德雷福斯（Stuart Dreyfus）（1987 年）一起被共同阐述，他是一位计算机科学家。粗略地说，这种攻击的核心思想是，人类的专业知识不是基于显式的、脱离实体的、机械化的符号信息操作（比如某种逻辑中的公式，或者某种贝叶斯网络中的概率），而且如果基于符号范式来构建具有这种专业知识的机器，那么这种努力注定会失败。德雷福斯攻击的起源是一种信念，即可以从海德格尔和梅洛-庞蒂等思想家对（如果你愿意这样说的话）基于符号的哲学（例如基于逻辑的理性主义传统的哲学，与所谓的大陆传统相对立）的批判中，对人工智能的理性主义传统进行批判。在进一步阅读和研究德雷福斯的著作之后，读者可以判断在一个越来越由进行符号推理的智能代理管理的信息驱动世界中，这种批判是否具有说服力（尽管远远不及人类水平）。

对于对人工智能哲学感兴趣的读者，除了吉姆·穆尔（在最近的一次演讲中，作为 2006 年巴尔维斯奖获得者在年度东部美国哲学协会会议上发表的“人工智能的未来五十年：未来科学研究与过去哲学批评”）所称的“三大”人工智能批评之外，还有大量的额外材料可供探索，其中许多材料都可以在网络上找到。《AIMA》的最后一章提供了一些针对“强人工智能”的额外论证的压缩概述，总体来说是一个不错的下一步。不用说，今天的人工智能哲学涉及的远不止上述三个众所周知的论点，而且不可避免地，明天的人工智能哲学将包括我们现在看不到的新的辩论和问题。因为无论机器变得多么聪明，人工智能哲学纯粹而简单地是一个增长的行业。随着机器与人类活动的匹配，这些“重大”问题只会吸引更多的关注。

## 9. 未来

如果过去的预测是任何指示，我们今天对明天的科学和技术唯一知道的是，它将与我们预测的完全不同。可以说，在人工智能的情况下，我们今天也可能特别知道进展将比大多数人预期的要慢得多。毕竟，在 1956 年的启动会议上（在本条目开头讨论），赫伯特·西蒙预测，能够与人类思维相匹配的思考机器“就在拐角处”（有关相关引文和信息性讨论，请参见_AIMA_的第一章）。事实证明，新世纪到来时，没有一台机器能够达到甚至是幼儿水平的对话能力。（请记住，当涉及到构建具有人类级智能的机器时，今天看来更好的预言者似乎是笛卡尔，而不是图灵。）尽管如此，尽管可能令人惊讶，但 20 世纪末的严肃思想家们继续对人工智能的进展发表非常乐观的预测。例如，汉斯·莫拉维克（1999 年）在他的《机器人：从纯粹的机器到超越的思维》中告诉我们，由于计算机硬件的速度每 18 个月翻一番（根据过去的情况，这符合摩尔定律），“第四代”机器人很快将在各个方面超过人类，从经营公司到写小说。故事是这样说的，这些机器人将进化到如此崇高的认知高度，我们将对它们如同单细胞生物对待我们今天一样。\[41]

莫拉维克绝不是唯一一个乐观的人：在人工智能 50 周年纪念会议上，吉姆·穆尔向参加 1956 年原始会议的五位思想家约翰·麦卡锡、马文·明斯基、奥利弗·塞尔弗里奇、雷·所罗门诺夫和特伦查德·摩尔提出了一个问题：“人类级别的人工智能是否会在未来 50 年内实现？”麦卡锡和明斯基坚定而毫不犹豫地回答肯定，所罗门诺夫似乎暗示人工智能在我们的物种似乎注定要自我毁灭的事实面前提供了一线希望。（塞尔弗里奇的回答有点神秘。摩尔则坚定而明确地回答否定，并宣称一旦他的计算机足够聪明，能够与他进行数学问题的对话，他可能会更认真地对待整个事业。）读者可以判断莫拉维克、麦卡锡和明斯基等人所作出的冒险预测的准确性。\[42]

读者在这方面的判断应该考虑到最近对所谓的“奇点”（我们简称为**S**）的严肃思考的惊人复兴，即人工智能超过人类智能的未来点，在那之后（故事如此）机器迅速变得越来越聪明，达到超人类水平的智能，而我们被困在有限思维的泥潭中，无法理解。有关**S**的广泛而平衡的分析，请参阅 Eden 等人（2013）。

对于不熟悉**S**文献的读者来说，他们可能会惊讶地了解到，在学术界，这个假设性事件不仅被认真对待，而且实际上已经成为广泛和频繁哲学思考的目标\[有关最近相关思想的尖刻导览，请参阅 Floridi（2015）]。是什么_论据_支持了**S**将来会发生的信念？目前有两个主要论据：熟悉的基于硬件的论据\[由 Moravec 提出，如上所述，最近又由 Kurzweil（2006）提出]；以及我们所知的数学家 I. J. Good（1965）提出的原始论据。此外，还有一个最近提出的与此相关的末日论据，由 Bostrom（2014）提出，这似乎预设了**S**将会发生。Good 的论证由 Chalmers（2010）进行了精心扩展和调整，他确认了论证的整理版本，论证如下：

* **前提 1**：将会有人造智能（由人工智能和类似的东西创造，且人造智能等于人工智能）。
* **前提 2**：如果存在人造智能，将会有人造智能+（由人造智能创造）。
* **前提 3**：如果存在 AI+，那么就会有 AI++（由 AI+ 创建）。
* **结论**：将会有 AI++（= S 将会发生）。

在这个论证中，“AI”是人类创造的人工智能，处于人类水平，“AI+”是超过人类水平的人工智能，“AI++”是构成**S**的超级智能。关键过程可能是一个机器类别被另一个机器类别创造出来。为了方便起见，我们添加了“HI”表示人类智能；核心思想是：HI 将创造 AI，后者具有与前者相同的智能水平；AI 将创造 AI+；AI+将创造 AI++；晋升可能会无限进行下去，但至少会进行足够长的时间，让我们像蚂蚁一样被神超越。

这个论证显然在形式上是有效的。它的三个前提是真实的吗？讨论这样一个问题将使我们远远超出本条目的范围。我们只指出，一个机器类别创造另一个更强大的机器类别的概念并不是一个透明的概念，Good 和 Chalmers 都没有对这个概念提供严格的解释，这个概念值得进行哲学分析。（当然，关于数学分析，一些存在。例如，众所周知，一个在水平 L 的计算机不可能创造出一个更高水平 L'的机器。例如，一个线性有界自动机无法创造出一个图灵机。）

好-查尔默斯论证在某种程度上有一种临床氛围；该论证并未说明 AI++类别的机器是善良的、恶意的还是慷慨的。许多人乐意用黑暗的悲观主义来填补这个空白。这里的经典之地无疑是比尔·乔伊（2000 年）的一篇广为人知的论文：“为什么未来不需要我们。”乔伊认为人类注定要灭亡，其中一个重要原因是它正忙于建造智能机器。他写道：

> 21 世纪的技术-基因、纳米技术和机器人（GNR）-是如此强大，以至于它们可以产生全新的事故和滥用。最危险的是，这些事故和滥用首次广泛地落入个人或小团体的手中。它们不需要大型设施或稀有原材料。仅凭知识就能使用它们。

因此，我们不仅有大规模毁灭性武器的可能性，还有知识驱动的大规模毁灭（KMD）的可能性，这种破坏力通过自我复制的能力得到了极大的放大

我认为毫不夸张地说，我们正处在极端邪恶进一步完善的边缘，这种邪恶的可能性远远超出了大规模杀伤性武器给国家带来的影响，而是对极端个体产生了令人惊讶和可怕的赋能

哲学家们对这个观点最感兴趣。乔伊的观点是什么呢？嗯，对他的论文给予如此多关注的一个重要原因是，像雷蒙德·库兹韦尔（2000 年）一样，乔伊在很大程度上依赖于一个由无政府主义者（西奥多·卡辛斯基）提出的论证。这个想法是，假设我们成功地建造出智能机器，我们将让它们为我们做大部分（如果不是全部）的工作。如果我们进一步允许机器为我们做决策——即使我们保留对机器的监督——我们最终将依赖它们到必须简单接受它们的决策的地步。但即使我们不允许机器做决策，这样的机器的控制很可能由一小部分精英掌握，他们将认为其他人类是多余的——因为机器可以完成任何需要的工作（乔伊，2000 年）。

这不是评估这个论证的地方。（话虽如此，无政府主义者及其支持者所推崇的模式显然是无效的。\[44]）事实上，许多读者无疑会觉得这样的地方不存在或将不存在，因为这里的推理是业余的。那么，专业哲学家对这个问题的推理又如何呢？

Bostrom 最近描绘了一个可能的黑暗未来。他指出，“第一个超级智能”可能具有以下能力：

> 形塑地球起源生命的未来，很可能具有非人类化的最终目标，并且很可能有追求无限资源获取的工具性原因。如果我们现在反思一下，人类由有用的资源（如方便地位于某处的原子）组成，并且我们依赖更多的本地资源，我们就可以看到，结果很可能是人类很快灭绝。（Bostrom 2014，第 416 页）

显然，在这种论证中最脆弱的前提是“第一个超级智能”确实会到来。在这里，Good-Chalmers 的论证或许提供了一个基础。

Searle（2014）认为 Bostrom 的书是误导和基本错误的，我们不需要担心。他的理由非常简单：机器没有意识；Bostrom 对恶意机器的前景感到担忧；恶意机器在定义上是有意识的机器；因此，Bostrom 的论证不成立。Searle 写道：

如果计算机能够驾驶飞机、开车和在国际象棋中获胜，那么它是否完全无意识并不重要。但是，如果我们担心一个恶意动机的超级智能会摧毁我们，那么恶意动机的真实性就很重要了。没有意识，它就不可能是真实的。

在这里，令人惊讶的是，西尔伯似乎没有意识到大多数人工智能工程师都很满意根据我们上面提出和解释的人工智能（AIMA）观点来构建机器：根据这个观点，机器只是简单地将感知映射到行动。在这个观点中，机器是否真正拥有欲望并不重要，重要的是它是否根据人工智能科学家工程师设计的形式上的欲望相关性来适当地行动。一个具有巨大破坏力的自主机器，即使在没有真正的、人类级别的、主观的欲望的情况下“决定”杀人，也不会只是一个麻烦。如果人工智能能够下国际象棋和《危险边缘》这个游戏，那么它肯定也能玩战争这个游戏。就像一个人类失败者指出在国际象棋比赛中胜利的机器没有意识一样，对于被机器杀死的人来说，指出这些机器没有意识也没有什么好处。（有趣的是，乔伊的论文的起源是与约翰·西尔伯和雷蒙德·库兹韦尔的一次非正式对话。根据乔伊的说法，西尔伯认为没有什么可担心的，因为他对明天的机器人不能有意识非常有信心。\[45]）

关于明天，我们可以_安全地_说一些事情。当然，除非发生一些灾难性事件（核战争或生物战争，全球经济萧条，陨石撞击地球等），我们现在知道人工智能将成功地产生人造_动物_。由于一些自然动物（例如骡子）甚至可以轻松地被训练为人类工作，因此可以推断，从头开始设计以满足我们目的的人造动物将被部署为为我们工作。实际上，许多目前由人类完成的工作肯定会由适当编程的人造动物来完成。举个任意的例子，很难相信未来商业司机不会是人造的。（事实上，戴姆勒已经在广告中宣传他们的汽车能够“自主驾驶”，使乘坐这些车辆的人类可以忽略道路并阅读。）其他例子包括：清洁工，邮递员，文员，军事侦察员，外科医生和飞行员。（至于清洁工，可能有相当数量的读者此刻家里有 iRobot 的机器人在清洁地毯。）很难看出这些工作与通常被认为是人性核心的属性是如何紧密相连的 - 这些属性对于人工智能来说是最难复制的。\[[46](https://plato.stanford.edu/entries/artificial-intelligence/notes.html#note-46)]

安迪·克拉克（2003）有另一个预测：人类将逐渐成为至少在相当程度上的人造人，得益于人工肢体和感官器官以及植入物。这一趋势的主要驱动力将是，虽然独立的人工智能通常是可取的，但当所需的智能水平较高时，很难进行工程设计。但是让人类“驾驶”智能程度较低的机器要容易得多，并且出于具体原因仍然非常有吸引力。另一个相关的预测是，人工智能将成为人类的认知假肢（福特等人，1997 年；霍夫曼等人，2001 年）。假肢观点认为人工智能是一个“伟大的平等器”，将导致社会分层减少，或许类似于印度-阿拉伯数字系统使算术对大众可用，以及古腾堡印刷术促进了读写能力的普及化。

即使论证在形式上是无效的，它也给我们留下了一个问题 - 关于人工智能和未来的基石性问题：人工智能是否会产生复制和超越人类认知的人造生物（正如库兹韦尔和乔伊所相信的）？还是这只是一个有趣的假设？

这不仅是科学家和工程师的问题，也是哲学家的问题。这是因为有两个原因。首先，旨在验证肯定答案的研究和开发必须包括哲学 - 原因根植于本文前面的部分。（例如，哲学是用机器术语建模人类命题态度的强大形式主义的地方。）其次，哲学家可能能够提供现在就能明确回答基石问题的论证。如果上述提到的反对“强”人工智能的三个论证（西尔的 CRA；哥德尔攻击；德雷福斯的论证）中的任何一个版本是正确的，那么当然人工智能将无法生产具有人类心智能力的机器。毫无疑问，未来不仅会有越来越聪明的机器，还会有关于这个问题的赞成和反对的新论证，即这种进展是否能达到笛卡尔所宣称的无法达到的人类水平。

## Bibliography

* Adams, E. W., 1996, _A Primer of Probability Logic_, Stanford, CA: CSLI.
* Almeida, J., Frade, M., Pinto, J. & de Sousa, S., 2011, _Rigorous Software Development: An Introduction to Program Verification_, New York, NY: Spinger.
* Alpaydin, E., 2014, _Introduction to Machine Learning_, Cambridge, MA: MIT Press.
* Amir, E. & Maynard-Reid, P., 1999, “Logic-Based Subsumption Architecture,” in _Proceedings of the 16th International Joint Conference on Artificial Intelligence (IJCAI-1999)_, (San Francisco, CA: MIT Morgan Kaufmann), pp. 147–152.
* Amir, E. & Maynard-Reid, P., 2000, “Logic-Based Subsumption Architecture: Empirical Evaluation,” in _Proceedings of the AAAI Fall Symposium on Parallel Architectures for Cognition_.
* Amir, E. & Maynard-Reid, P., 2001, “LiSA: A Robot Driven by Logical Subsumption,” in _Proceedings of the Fifth Symposium on the Logical Formalization of Commonsense Reasoning_, (New York, NY).
* Anderson, C. A., 1983, “The Paradox of the Knower,” _The Journal of Philosophy,_ 80.6: 338–355.
* Anderson, J. & Lebiere, C., 2003, “The Newell Test for a Theory of Cognition,” _Behavioral and Brain Sciences_, 26: 587–640.
* Ashcraft, M., 1994, _Human Memory and Cognition_, New York, NY: HarperCollins.
* Arkin, R., 2009, _Governing Lethal Behavior in Autonomous Robots_, London: Chapman and Hall/CRC Imprint, Taylor and Francis Group.
* Arkoudas, K. & Bringsjord, S., 2005, “Vivid: A Framework for Heterogeneous Problem Solving,” _Artificial Intelligence_, 173.15: 1367–1405.
* Arkoudas, K. & Bringsjord, S., 2005, “Metareasoning for Multi-agent Epistemic Logics,” in _Fifth International Conference on Computational Logic In Multi-Agent Systems (CLIMA 2004)_, in the series _Lecture Notes in Artificial Intelligence (LNAI)_, volume 3487, New York, NY: Springer-Verlag, pp. 111–125.
* Arkoudas, K., 2000, _Denotational Proof Languages_, PhD dissertation, Massachusetts Institute of Technology (Computer Science).
* Baader, F., Calvanese, D., McGuinness, D. L., Nardi, D., & Patel-Schneider, P. F., eds., 2003, _The Description Logic Handbook: Theory, Implementation, and Applications_, New York, NY: Cambridge University Press.
* Smith, B., Ashburner, M., Rosse, C., Bard, J., Bug, W., Ceusters, W., Goldberg, L. J., Eilbeck, K., Ireland, A., Mungall, C. J., The OBI Consortium, Leontis, N., Rocca-Serra, P., Ruttenberg, A., Sansone, S., Scheuermann, R. H., Shah, N., Whetzel, P. L. & Lewis, S., 2007, “The OBO Foundry: Coordinated Evolution of Ontologies to Support Biomedical Data Integration,” _Nature Biotechnology_ 25, 1251–1255.
* Barwise, J. & Etchemendy, J., 1999, _Language, Proof, and Logic_, New York, NY: Seven Bridges Press.
* Barwise, J. & Etchemendy, J., 1995, “Heterogeneous Logic,” in _Diagrammatic Reasoning: Cognitive and Computational Perspectives_, J. Glasgow, N.H. Narayanan, & B. Chandrasekaran, eds., Cambridge, MA: MIT Press, pp. 211–234.
* Baldi, P., Sadowski P. & Whiteson D., 2014, “Searching for Exotic Particles in High-energy Physics with Deep Learning,” _Nature Communications_. \[[Available online](http://www.nature.com/ncomms/2014/140702/ncomms5308/full/ncomms5308.html)]
* Barwise, J. & Etchemendy, J., 1994, _Hyperproof_, Stanford, CA: CSLI.
* Barwise, J. & Etchemendy, J., 1990, “Infons and Inference,” in _Situation Theory and its Applications, (Vol 1)_, Cooper, Mukai, and Perry (eds), CSLI Lecture Notes #22, CSLI Press, pp. 33–78.
* Bello, P. & Bringsjord S., 2013, “On How to Build a Moral Machine,” _Topoi,_ 32.2: 251–266.
* Bengio, Y., Goodfellow, I., & Courville, A., 2016, _Deep Learning_, Cambridge: MIT Press. \[[Available online](http://www.deeplearningbook.org/)]
* Bengio, Y., Courville, A. & Vincent, P., 2013, “Representation Learning: A Review and New Perspectives,” _Pattern Analysis and Machine Intelligence, IEEE Transactions,_ 35.8: 1798–1828.
* Berners-Lee, T., Hendler, J. & Lassila, O., 2001, “The Semantic Web,” _Scientific American,_ 284: 34–43.
* Bishop, M. & Preston, J., 2002, _Views into the Chinese Room: New Essays on Searle and Artificial Intelligence_, Oxford, UK: Oxford University Press.
* Boden, M., 1994, “Creativity and Computers,” in _Artificial Intelligence and Computers_, T. Dartnall, ed., Dordrecht, The Netherlands: Kluwer, pp. 3–26.
* Boolos, G. S., Burgess, J.P., & Jeffrey., R.C., 2007, _Computability and Logic 5th edition,_ Cambridge: Cambridge University Press.
* Bostrom, N., 2014, _Superintelligence: Paths, Dangers, Strategies_, Oxford, UK: Oxford University Press.
* Bowie, G.L., 1982, “Lucas’ Number is Finally Up,” _Journal of Philosophical Logic_, 11: 279–285.
* Brachman, R. & Levesque, H., 2004, _Knowledge Representation and Reasoning_, San Francisco, CA: Morgan Kaufmann/Elsevier.
* Bringsjord, S., Arkoudas K. & Bello P., 2006, “Toward a General Logicist Methodology for Engineering Ethically Correct Robots,” IEEE Intelligent Systems, 21.4: 38–44.
* Bringsjord, S. & Ferrucci, D., 1998, “Logic and Artificial Intelligence: Divorced, Still Married, Separated…?” _Minds and Machines_, 8: 273–308.
* Bringsjord, S. & Schimanski, B., 2003, “What is Artificial Intelligence? Psychometric AI as an Answer,” _Proceedings of the 18th International Joint Conference on Artificial Intelligence (IJCAI-2003)_, (San Francisco, CA: MIT Morgan Kaufmann), pp. 887–893.
* Bringsjord, S. & Ferrucci, D., 2000, _Artificial Intelligence and Literary Creativity: Inside the Mind of Brutus, a Storytelling Machine_, Mahwah, NJ: Lawrence Erlbaum.
* Bringsjord, S. & van Heuveln, B., 2003, “The Mental Eye Defense of an Infinitized Version of Yablo’s Paradox,” _Analysis_ 63.1: 61–70.
* Bringsjord S. & Xiao, H., 2000, “A Refutation of Penrose’s Gödelian Case Against Artificial Intelligence,” _Journal of Experimental and Theoretical Artificial Intelligence_, 12: 307–329.
* Bringsjord, S. & Zenzen, M., 2002, “Toward a Formal Philosophy of Hypercomputation,” _Minds and Machines_, 12: 241–258.
* Bringsjord, S., 2000, “Animals, Zombanimals, and the Total Turing Test: The Essence of Artificial Intelligence,” _Journal of Logic, Language, and Information_, 9: 397–418.
* Bringsjord, S., 1998, “Philosophy and ‘Super’ Computation,” _The Digital Phoenix: How Computers are Changing Philosophy_, J. Moor and T. Bynam, eds., Oxford, UK: Oxford University Press, pp. 231–252.
* Bringsjord, S., 1991, “Is the Connectionist-Logicist Clash one of AI’s Wonderful Red Herrings?” _Journal of Experimental & Theoretical AI_, 3.4: 319–349.
* Bringsjord, S., Govindarajulu N. S., Eberbach, E. & Yang, Y., 2012, “Perhaps the Rigorous Modeling of Economic Phenomena Requires Hypercomputation,” _International Journal of Unconventional Computing,_ 8.1: 3–32. \[[Preprint available online](http://kryten.mm.rpi.edu/SB\*NSG\*EE\*YY\*28-9-2010.pdf)]
* Bringsjord, S., 2011, “Psychometric Artificial Intelligence,” _Journal of Experimental and Theoretical Artificial Intelligence_, 23.3: 271–277.
* Bringsjord, S. & Govindarajulu N. S., 2012, “Given the Web, What is Intelligence, Really?” _Metaphilosophy_ 43.12: 464–479.
* Brooks, R. A., 1991, “Intelligence Without Representation,” _Artificial Intelligence_, 47: 139–159.
* Browne, C. B., Powley, E. & Whitehouse, D., 2012, “A Survey of Monte Carlo Tree Search Methods,” _A Survey of Monte Carlo Tree Search Methods_, 4.1: 1–43.
* Buchanan, B. G., 2005, “A (Very) Brief History of Artificial Intelligence,” _AI Magazine_, 26.4: 53–60.
* Carroll, L., 1958, _Symbolic Logic; Game of Logic_, New York, NY: Dover.
* Cassimatis, N., 2006, “Cognitive Substrate for Human-Level Intelligence,” _AI Magazine_, 27.2: 71–82.
* Chajed, T., Chen, H., Chlipala, A., Kaashoek, F., Zeldovich, N., & Ziegler, D., 2017, “Research Highlight: Certifying a File System using Crash Hoare Logic: Correctness in the Presence of Crashes,” _Communications of the ACM (CACM),_ 60.4: 75–84.
* Chalmers, D., 2010, “The Singularity: A Philosophical Analysis,” _Journal of Consciousness Studies_, 17: 7–65.
* Charniak, E., 1993, _Statistical Language Learning_, Cambridge: MIT Press.
* Charniak, E. & McDermott, D., 1985, _Introduction to Artificial Intelligence_, Reading, MA: Addison Wesley.
* Chellas, B., 1980, _Modal Logic: An Introduction_, Cambridge, UK: Cambridge University Press.
* Chisholm, R., 1957, _Perceiving_, Ithaca, NY: Cornell University Press.
* Chisholm, R., 1966, _Theory of Knowledge_, Englewood Cliffs, NJ: Prentice-Hall.
* Chisholm, R., 1977, _Theory of Knowledge 2nd ed_, Englewood Cliffs, NJ: Prentice-Hall.
* Clark, A., 2003, _Natural-Born Cyborgs_, Oxford, UK: Oxford University Press.
* Clark, M. H., 2010, _Cognitive Illusions and the Lying Machine: A Blueprint for Sophistic Mendacity_, PhD dissertation, Rensselaer Polytechnic Institute (Cognitive Science).
* Copeland, B. J., 1998, “Super Turing Machines,” _Complexity_, 4: 30–32.
* Copi, I. & Cohen, C., 2004, _Introduction to Logic_, Saddle River, NJ: Prentice-Hall.
* Dennett, D., 1998, “Artificial Life as Philosophy,” in his _Brainchildren: Essays on Designing Minds_, Cambridge, MA: MIT Press, pp. 261–263.
* Dennett, D., 1994, “The Practical Requirements for Making a Conscious Robot,” _Philosophical Transactions of the Royal Society of London_, 349: 133–146.
* Dennett, D., 1979, “Artificial Intelligence as Philosophy and as Psychology,” _Philosophical Perspectives in Artificial Intelligence_, M. Ringle, ed., Atlantic Highlands, NJ: Humanities Press, pp. 57–80.
* Descartes, 1637, R., in Haldane, E. and Ross, G.R.T., translators, 1911, _The Philosophical Works of Descartes, Volume 1,_ Cambridge, UK: Cambridge University Press.
* Dick, P. K., 1968, _Do Androids Dream of Electric Sheep?_, New York, NY: Doubleday.
* Domingos, P., 2012, “A Few Useful Things to Know about Machine Learning,” _Communications of the ACM_, 55.10: 78–87.
* Dreyfus, H., 1972, _What Computers Can’t Do_, Cambridge, MA: MIT Press.
* Dreyfus, H., 1992, _What Computers Still Can’t Do_, Cambridge, MA: MIT Press.
* Dreyfus, H. & Dreyfus, S., 1987, _Mind Over Machine: The Power of Human Intuition and Expertise in the Era of the Computer_, New York, NY: Free Press.
* Ebbinghaus, H., Flum, J. & Thomas, W., 1984, _Mathematical Logic_, New York, NY: Springer-Verlag.
* Eden, A., Moor, J., Soraker, J. & Steinhart, E., 2013, _Singularity Hypotheses: A Scientific and Philosophical Assessment_, New York, NY: Springer.
* España-Bonet, C., Enache, R., Slaski, A., Ranta, A., Màrquez L. & Gonzàlez, M., 2011, “Patent Translation within the MOLTO project,” in _Proceedings of the 4th Workshop on Patent Translation, MT Summit XIII_, pp. 70–78.
* Evans, G., 1968, “A Program for the Solution of a Class of Geometric-Analogy Intelligence-Test Questions,” in M. Minsky, ed., _Semantic Information Processing_, Cambridge, MA: MIT Press, pp. 271–353.
* Fagin, R., Halpern, J. Y., Moses, Y. & Vardi, M., 2004, _Reasoning About Knowledge_, Cambridge, MA: MIT Press.
* Ferrucci, D. & Lally, A., 2004, “UIMA: An Architectural Approach to Unstructured Information Processing in the Corporate Research Environment,” _Natural Language Engineering_, 10.3–4: 327–348. Cambridge, UK: Cambridge University Press.
* Ferrucci, D., Brown, E., Chu-Carroll, J., Fan, J., Gondek, D., Kalyanpur, A., Lally, A., Murdock, J., Nyberg, E., Prager, J., Schlaefer, N. & Welty, C., 2010, “Building Watson: An Overview of the DeepQA Project,” _AI Magazine_, 31.3: 59–79.
* Finnsson, H., 2012, “Generalized Monte-Carlo Tree Search Extensions for General Game Playing,” in _Proceedings of the Twenty-Sixth AAAI Conference on Artificial Intelligence (AAAI-2012)_, Toronto, Canda, pp. 1550–1556.
* Fitelson, B., 2005, “Inductive Logic,” in Pfeifer, J. and Sarkar, S., eds., _Philosophy of Science: An Encyclopedia_, London, UK: Routledge, pp. 384–394.
* Floridi, L., 2015, “Singularitarians, AItheists, and Why the Problem with Artificial Intelligence is H.A.L. (Humanity At Large), not HAL,” _APA Newsletter: Philosophy and Computers_, 14.2: 8–11.
* Foot, P., 1967, “The Problem of Abortion and the Doctrine of the Double Effect,” _Oxford Review_, 5: 5–15.
* Forbus, K. D. & Hinrichs, T. R., 2006, “Companion Cognitive Systems: A Step toward Human-Level AI,” _AI Magazine,_ 27.2: 83.
* Ford, K. M., Glymour C. & Hayes P., 1997, “On the Other Hand … Cognitive Prostheses,” _AI Magazine,_ 18.3: 104.
* Friedland, N., Allen, P., Matthews, G., Witbrock, M., Baxter, D., Curtis, J., Shepard, B., Miraglia, P., Angele, J., Staab, S., Moench, E., Oppermann, H., Wenke, D., Israel, D., Chaudhri, V., Porter, B., Barker, K., Fan, J., Yi Chaw, S., Yeh, P., Tecuci, D. & Clark, P., 2004, “Project Halo: Towards a Digital Aristotle,” _AI Magazine_, 25.4: 29–47.
* Genesereth, M., Love, N. & Pell B., 2005, “General Game Playing: Overview of the AAAI Competition,” _AI Magazine_, 26.2: 62–72. \[[Available online](https://www.aaai.org/ocs/index.php/IJCAI/IJCAI-09/paper/viewFile/566/775)]
* Ginsberg, M., 1993, _Essentials of Artificial Intelligence_, New York, NY: Morgan Kaufmann.
* Glymour, G., 1992, _Thinking Things Through_, Cambridge, MA: MIT Press.
* Goertzel, B. & Pennachin, C., eds., 2007, _Artificial General Intelligence_, Berlin, Heidelberg: Springer-Verlag.
* Gold, M., 1965, “Limiting Recursion,” _Journal of Symbolic Logic_, 30.1: 28–47.
* Goldstine, H. & von Neumann, J., 1947, “Planning and Coding of Problems for an Electronic Computing Instrument,” _IAS Reports_ Institute for Advanced Study, Princeton, NJ. \[This remarkable work is [available online](https://library.ias.edu/files/pdfs/ecp/planningcodingof0103inst.pdf) from the Institute for Advanced Study. Please note that this paper is Part II of a three-volume set. The first volume was devoted to a preliminary discussion, and the first author on it was Arthur Burks, joining Goldstine and von Neumann.]
* Good, I., 1965, “Speculations Concerning the First Ultraintelligent Machines,” in _Advances in Computing_ (vol. 6), F. Alt and M. Rubinoff, eds., New York, NY: Academic Press, pp. 31–38.
* Govindarajulu, N. S., Bringsjord, S. & Licato J., 2013, “On Deep Computational Formalization of Natural Language,” in _Proceedings of the Workshop “Formalizing Mechanisms for Artificial General Intelligence and Cognition (Formal MAGiC),”_ Osnabrück, Germany: PICS.
* Govindarajulu, N. S., & Bringsjord, S., 2015, “Ethical Regulation of Robots Must Be Embedded in Their Operating Systems” in Trappl, R., ed., _A Construction Manual for Robot’s Ethical Systems: Requirements, Methods, Implementations_, Berlin, DE: Springer.
* Govindarajulu, N. S., & Bringsjord, S., 2017, “On Automating the Doctrine of Double Effect,” in _Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence (IJCAI-17)_, pp. 4722–4730. doi:10.24963/ijcai.2017/658
* Granger, R., 2004a, “Derivation and Analysis of Basic Computational Operations of Thalamocortical Circuits,” _Journal of Cognitive Neuroscience_ 16: 856–877.
* Granger, R., 2004b, “Brain Circuit Implementation: High-precision Computation from Low-Precision Components,” in _Toward Replacement Parts for the Brain_, T. Berger and D. Glanzman, eds., Cambridge, MA: MIT Press, pp. 277–294.
* Griewank, A., 2000, _Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation_, Philadlphia, PA: Society for Industrial and Applied Mathematics (SIAM).
* Guizzo, E., 2011, “How Google’s Self-driving Car Works,” _IEEE Spectrum Online_. [\[Available online\]](https://spectrum.ieee.org/automaton/robotics/artificial-intelligence/how-google-self-driving-car-works)
* Hailperin, T., 1996, _Sentential Probability Logic: Origins, Development, Current Status, and Technical Applications,_ Bethlehem, United States: Lehigh University Press.
* Hailperin, T., 2010, _Logic with a Probability Semantics,_ Bethlehem, United States: Lehigh University Press.
* Halpern, J. Y., 1990, “An Analysis of First-order Logics of Probability,” _Artificial Intelligence,_ 46: 311–350.
* Halpern, J., Harper, R., Immerman, N., Kolaitis, P. G., Vardi, M. & Vianu, V., 2001, “On the Unusual Effectiveness of Logic in Computer Science,” _The Bulletin of Symbolic Logic_, 7.2: 213–236.
* Hamkins, J. & Lewis, A., 2000, “Infinite Time Turing Machines,” _Journal of Symbolic Logic_, 65.2: 567–604.
* Harnad, S., 1991, “Other Bodies, Other Minds: A Machine Incarnation of an Old Philosophical Problem,” _Minds and Machines_, 1.1: 43–54.
* Haugeland, J., 1985, _Artificial Intelligence: The Very Idea_, Cambridge, MA: MIT Press.
* Hendler, J. & Jennifer G., 2008, “Metcalfe’s Law, Web 2.0, and the Semantic Web,” _Web Semantics: Science, Services and Agents on the World Wide Web,_ 6.1: 14–20.
* Hinton, G., Deng, L., Yu, D., Dahl, G.E., Mohamed, A. R., Jaitly, N., Senior, A., Vanhoucke, V., Nguyen, P., Sainath, T. & Kingsbury, B., 2012, “Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups,” _IEEE Signal Processing Magazine_, 29.6: 82–97.
* Hoffman, R. R., Hayes, P. J. & Ford, K. M., 2001, “Human-Centered Computing: Thinking In and Out of the Box,” _IEEE Intelligent Systems_, 16.5: 76–78.
* Hoffman, R. R., Bradshaw J. M., Hayes P. J. & Ford K. M., 2003, “ The Borg Hypothesis,” _IEEE Intelligent Systems_, 18.5: 73–75.
* Hofstadter, D. & McGraw, G., 1995, “Letter Spirit: Esthetic Perception and Creative Play in the Rich Microcosm of the Roman Alphabet,” in Hofstadter’s _Fluid Concepts and Creative Analogies: Computer Models of the Fundamental Mechanisms of Thought_, New York, NY: Basic Books, pp. 407–488.
* Hornik, K., Stinchcombe, M. & White, H., 1989, “Multilayer Feedforward Networks are Universal Approximators,” _Neural Networks_, 2.5: 359–366.
* Hutter, M., 2005, _Universal Artificial Intelligence_, Berlin: Springer.
* Joy, W., 2000, “Why the Future Doesn’t Need Us,” _Wired_ 8.4. \[[Available online](https://www.wired.com/2000/04/joy-2/)]
* Kahneman, D., 2013. _Thinking, Fast and Slow_, New York, NY: Farrar, Straus, and Giroux.
* Kaufmann, M., Manolios, P. & Moore, J. S., 2000, _Computer-Aided Reasoning: ACL2 Case Studies_, Dordrecht, The Netherlands: Kluwer Academic Publishers.
* Klenk, M., Forbus, K., Tomai, E., Kim,H. & Kyckelhahn, B., 2005, “Solving Everyday Physical Reasoning Problems by Analogy using Sketches,” in _Proceedings of 20th National Conference on Artificial Intelligence_ (AAAI-05), Pittsburgh, PA.
* Kolata, G., 1996, “Computer Math Proof Shows Reasoning Power,” in _New York Times_. \[[Availabe online](http://www.nytimes.com/library/cyber/week/1210math.html)]
* Koller, D., Levy, A. & Pfeffer, A., 1997, “P-CLASSIC: A Tractable Probablistic Description Logic,” in _Proceedings of the AAAI 1997 Meeting_, 390–397.
* Kurzweil, R., 2006, _The Singularity Is Near: When Humans Transcend Biology_, New York, NY: Penguin USA.
* Kurzweil, R., 2000, _The Age of Spiritual Machines: When Computers Exceed Human Intelligence_, New York, NY: Penguin USA.
* LaForte, G., Hayes P. & Ford, K., 1998, “Why Gödel’s Theorem Cannot Refute Computationslism,” _Artificial Intelligence_, 104: 265–286.
* Laird, J. E., 2012, _The Soar Cognitive Architecture_, Cambridge, MA: MIT Press.
* Laird, J. & VanLent M., 2001, “Human-level AI’s Killer Application: Interactive Computer Games,” _AI Magazine_ 22.2:15–26.
* LeCun, Y., Bengio, Y. & Hinton G., 2015, “Deep Learning,” _Nature_, 521: 436–444.
* Lenat, D., 1983, “EURISKO: A Program that Learns New Heuristics and Domain Concepts,” _Artificial Intelligence_, 21(1-2): 61–98. doi:10.1016/s0004-3702(83)80005-8
* Lenat, D., & Guha, R. V., 1990, _Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project_, Reading, MA: Addison Wesley.
* Lenzen, W., 2004, “Leibniz’s Logic,” in Gabbay, D., Woods, J. and Kanamori, A., eds., _Handbook of the History of Logic_, Elsevier, Amsterdam, The Netherlands, pp. 1–83.
* Lewis, H. & Papadimitriou, C., 1981, _Elements of the Theory of Computation_, Prentice Hall, Englewood Cliffs, NJ: Prentice Hall.
* Litt, A., Eliasmith, C., Kroon, F., Weinstein, S. & Thagard, P., 2006, “Is the Brain a Quantum Computer?” _Cognitive Science_ 30: 593–603.
* Lucas, J. R., 1964, “Minds, Machines, and Gödel,” in _Minds and Machines_, A. R. Anderson, ed., Prentice-Hall, NJ: Prentice-Hall, pp. 43–59.
* Luger, G., 2008, _Artificial Intelligence: Structures and Strategies for Complex Problem Solving_, New York, NY: Pearson.
* Luger, G. & Stubblefield, W., 1993, _Artificial Intelligence: Structures and Strategies for Complex Problem Solving_, Redwood, CA: Benjamin Cummings.
* Lopez, A., 2008, “Statistical Machine Translation,” _ACM Computing Surveys_, 40.3: 1–49.
* Malle, B. F., Scheutz, M., Arnold, T., Voiklis, J. & Cusimano, C., 2015, “Sacrifice One For the Good of Many?: People Apply Different Moral Norms to Human and Robot Agents,” in _Proceedings of the Tenth Annual ACM/IEEE International Conference on Human-Robot Interaction (HRI ’15)_ (New York, NY: ACM), pp. 117–124.
* Manzano, M., 1996, _Extensions of First Order Logic_, Cambridge, UK: Cambridge University Press.
* Marcus, G., 2013, “Why Can’t My Computer Understand Me?,” in _The New Yorker_, August 2013. \[[Available online](http://www.newyorker.com/online/blogs/elements/2013/08/why-cant-my-computer-understand-me.html)]
* McCarthy, J. & Hayes, P., 1969, “Some Philosophical Problems from the Standpoint of Artificial Intelligence,” in _Machine Intelligence 4_, B. Meltzer and D. Michie, eds., Edinburgh: Edinburgh University Press, 463–502.
* Mueller, E., 2006, _Commonsense Reasoning_, San Francisco, CA: Morgan Kaufmann.
* Murphy, K. P., 2012, _Machine Learning: A Probabilistic Perspective_, Cambridge, MA: MIT Press.
* Minsky, M. & Pappert, S., 1969, _Perceptrons: An Introduction to Computational Geometry_, Cambridge, MA: MIT Press.
* Montague, R., 1970, “Universal Grammar,” _Theoria,_ 36, 373–398.
* Moor, J., 2006, “The Nature, Importance, and Difficulty of Machine Ethics”, _IEEE Intelligent Systems_ 21.4: 18–21.
* Moor, J., 1985, “What is Computer Ethics?” _Metaphilosophy_ 16.4: 266–274.
* Moor, J., ed., 2003, _The Turing Test: The Elusive Standard of Artificial Intelligence_, Dordrecht, The Netherlands: Kluwer Academic Publishers.
* Moravec, H., 1999, _Robot: Mere Machine to Transcendant Mind_, Oxford, UK: Oxford University Press,
* Naumowicz, A. & Kornilowicz., A., 2009, “A Brief Overview of Mizar,” in _Theorem Proving in Higher Order Logics,_ S. Berghofer, T. Nipkow, C. Urban & M. Wenzel, eds., Berlin: Springer, pp. 67–72.
* Newell, N., 1973, “You Can’t Play 20 Questions with Nature and Win: Projective Comments on the Papers of this Symposium”, in _Visual Information Processing_, W. Chase, ed., New York, NY: Academic Press, pp. 283–308.
* Nilsson, N., 1998, _Artificial Intelligence: A New Synthesis_, San Francisco, CA: Morgan Kaufmann.
* Nilsson, N., 1987, _Principles of Artificial Intelligence_, New York, NY: Springer-Verlag.
* Nilsson, N., 1991, “Logic and Artificial Intelligence,” _Artificial Intelligence_, 47: 31–56.
* Nozick, R., 1970, “Newcomb’s Problem and Two Principles of Choice,” in _Essays in Honor of Carl G. Hempel_, N. Rescher, ed., Highlands, NJ: Humanities Press, pp. 114–146. This appears to be the very first published treatment of NP – though the paradox goes back to its creator: William Newcomb, a physicist.
* Osherson, D., Stob, M. & Weinstein, S., 1986, _Systems That Learn_, Cambridge, MA: MIT Press.
* Pearl, J., 1988, _Probabilistic Reasoning in Intelligent Systems_, San Mateo, CA: Morgan Kaufmann.
* Pennington, J., Socher R., & Manning C. D., 2014, “GloVe: Global Vectors for Word Representation,” in _Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP 2014)_, pp. 1532–1543. \[[Available online](http://www.aclweb.org/anthology/D14-1162)]
* Penrose, R., 1989, _The Emperor’s New Mind_, Oxford, UK: Oxford University Press.
* Penrose, R., 1994, _Shadows of the Mind_, Oxford, UK: Oxford University Press.
* Penrose, R., 1996, “Beyond the Doubting of a Shadow: A Reply to Commentaries on _Shadows of the Mind,_” _Psyche_, 2.3. This paper is available [online.](http://www.calculemus.org/MathUniversalis/NS/10/01penrose.html)
* Pereira, L., & Saptawijaya A., 2016, _Programming Machine Ethics,_ Berlin, Germany: Springer
* Pinker, S., 1997, _How the Mind Works,_ New York, NY: Norton.
* Pollock, J., 2006, _Thinking about Acting: Logical Foundations for Rational Decision Making,_ Oxford, UK: Oxford University Press.
* Pollock, J., 2001, “Defeasible Reasoning with Variable Degrees of Justification,” _Artificial Intelligence_, 133, 233–282.
* Pollock, J., 1995, _Cognitive Carpentry: A Blueprint for How to Build a Person_, Cambridge, MA: MIT Press.
* Pollock, J., 1992, “How to Reason Defeasibly,” _Artificial Intelligence_, 57, 1–42.
* Pollock, J., 1989, _How to Build a Person: A Prolegomenon_, Cambridge, MA: MIT Press.
* Pollock, J., 1974, _Knowledge and Justification_, Princeton, NJ: Princeton University Press.
* Pollock, J., 1967, “Criteria and our Knowledge of the Material World,” _Philosophical Review_, 76, 28–60.
* Pollock, J., 1965, _Analyticity and Implication,_ PhD dissertation, University of California at Berkeley (Philosophy).
* Potter, M.D., 2004, _Set Theory and its Philosophy_, Oxford, UK: Oxford University Press
* Preston, J. & Bishop, M., 2002, _Views into the Chinese Room: New Essays on Searle and Artificial Intelligence_, Oxford, UK: Oxford University Press.
* Putnam, H., 1965, “Trial and Error Predicates and a Solution to a Problem of Mostowski,” _Journal of Symbolic Logic,_ 30.1, 49–57.
* Putnam, H., 1963, “Degree of Confirmation and Inductive Logic,” in _The Philosophy of Rudolf Carnap_, Schilipp, P., ed., Open Court, pp. 270–292.
* Rajat, R., Anand, M. & Ng, A. Y., 2009, “Large-scale Deep Unsupervised Learning Using Graphics Processors,” in _Proceedings of the 26th Annual International Conference on Machine Learning_, ACM, pp. 873–880.
* Rapaport, W., 1988, “Syntactic Semantics: Foundations of Computational Natural-Language Understanding,” in _Aspects of Artificial Intelligence_, J. H. Fetzer ed., Dordrecht, The Netherlands: Kluwer Academic Publishers, 81–131.
* Rapaport, W. & Shapiro, S., 1999, “Cognition and Fiction: An Introduction,” _Understanding Language Understanding: Computational Models of Reading_, A. Ram & K. Moorman, eds., Cambridge, MA: MIT Press, 11–25. \[[Available online](https://cse.buffalo.edu/\~rapaport/Papers/fiction.ashwin.pdf)]
* Reeke, G. & Edelman, G., 1988, “Real Brains and Artificial Intelligence,” in _The Artificial Intelligence Debate: False Starts, Real Foundations_, Cambridge, MA: MIT Press, pp. 143–173.
* Richardson, M. & Domingos, P., 2006, “Markov Logic Networks,” _Machine Learning,_ 62.1–2:107–136.
* Robertson, G. & Watson, I., 2015, “A Review of Real-Time Strategy Game AI,” _AI Magazine_, 35.4: 75–104.
* Rosenschein, S. & Kaelbling, L., 1986, “The Synthesis of Machines with Provable Epistemic Properties,” in _Proceedings of the 1986 Conference on Theoretical Aspects of Reasoning About Knowledge_, San Mateo, CA: Morgan Kaufmann, pp. 83–98.
* Rumelhart, D. & McClelland, J., 1986, eds., _Parallel Distributed Processing_, Cambridge, MA: MIT Press.
* Russell, S., 1997, “Rationality and Intelligence,” _Artificial Intelligence_, 94: 57–77. \[[Version available online from author](https://people.eecs.berkeley.edu/\~russell/papers/aij-cnt.pdf)]
* Russell, S. & Norvig, P., 1995, _Artificial Intelligence: A Modern Approach_, Saddle River, NJ: Prentice Hall.
* Russell, S. & Norvig, P., 2002, _Artificial Intelligence: A Modern Approach 2nd edition_, Saddle River, NJ: Prentice Hall.
* Russell, S. & Norvig, P., 2009, _Artificial Intelligence: A Modern Approach 3rd edition_, Saddle River, NJ: Prentice Hall.
* Rychtyckyj, N. & Plesco, C., 2012, “Applying Automated Language Translation at a Global Enterprise Level,” _AI Magazine_, 34.1: 43–54.
* Scanlon, T. M., 1982, “Contractualism and Utilitarianism,” in A. Sen and B. Williams, eds., _Utilitarianism and Beyond,_ Cambridge: Cambridge University Press, pp. 103–128.
* Schank, R., 1972, “Conceptual Dependency: A Theory of Natural Language Understanding,” _Cognitive Psychology_, 3.4: 532–631.
* Schaul, T. & Schmidhüber, J., 2010, “Metalearning,” _Scholarpedia_ 5(6): 4650. URL: http://www.scholarpedia.org/article/Metalearning
* Schmidhüber, J., 2009, “Ultimate Cognition à la Gödel,” _Cognitive Computation_ 1.2: 177–193.
* Searle, J., 1997, _The Mystery of Consciousness_, New York, NY: New York Review of Books.
* Searle, J., 1980, “Minds, Brains and Programs,” _Behavioral and Brain Sciences_, 3: 417–424.
* Searle, J., 1984, _Minds, Brains and Science,_ Cambridge, MA: Harvard University Press. The Chinese Room Argument is covered in Chapter Two, “Can Computers Think?”.
* Searle, J., 2014, “What Your Computer Can’t Know,” _New York Review of Books_, October 9.
* Shapiro, S., 2000, “An Introduction to SNePS 3,” in _Conceptual Structures: Logical, Linguistic, and Computational Issues. Lecture Notes in Artificial Intelligence 1867_, B. Ganter & G. W. Mineau, eds., Springer-Verlag, 510–524.
* Shapiro, S., 2003, “Mechanism, Truth, and Penrose’s New Argument,” _Journal of Philosophical Logic_, 32.1: 19–42.
* Siegelmann, H., 1999, _Neural Networks and Analog Computation: Beyond the Turing Limit_, Boston, MA: Birkhauser.
* Siegelmann, H. & and Sontag, E., 1994, “Analog Computation Via Neural Nets,” _Theoretical Computer Science_, 131: 331–360.
* Silver, D., Huang, A., Maddison, C. J., Guez, A., Sifre, L., van den Driessche, G., Schrittwieser, J., Antonoglou, I., Panneershelvam, V., Lanctot, M., Dieleman, S., Grewe, D., Nham, J., Kalchbrenner, N., Sutskever, I., Lillicrap, T., Leach, M., Kavukcuoglu, K., Graepel T. & Hassabis D., 2016, “Mastering the Game of Go with Deep Neural Networks and Tree Search,” _Nature_, 529: 484–489.
* Shin, S-J, 2002, _The Iconic Logic of Peirce’s Graphs,_ Cambridge, MA: MIT Press.
* Smolensky, P., 1988, “On the Proper Treatment of Connectionism,” _Behavioral & Brain Sciences_, 11: 1–22.
* Somers, J., 2013, “The Man Who Would Teach Machines to Think,” in _The Atlantic_. \[[Available online](http://theatlantic.com/magazine/archive/2013/11/the-man-who-would-teach-machines-to-think/309529/)]
* Stanovich, K. & West, R., 2000, “Individual Differences in Reasoning: Implications for the Rationality Debate,” _Behavioral and Brain Sciences_, 23.5: 645–665.
* Strzalkowski, T. & Harabagiu, M. S., 2006, eds., _Advances in Open Domain Question Answering_; in the series Text, Speech and Language Technology, volume 32, Dordrecht, The Netherlands: Springer-Verlag.
* Sun, R., 2002, _Duality of the Mind: A Bottom Up Approach Toward Cognition_, Mahwah, NJ: Lawrence Erlbaum.
* Sun, R., 1994, _Integrating Rules and Connectionism for Robust Commonsense Reasoning_, New York, NY: John Wiley and Sons.
* Sutton R. S. & Barto A. G., 1998, _Reinforcement Learning: An Introduction_, Cambridge, MA: MIT Press.
* Szegedy, C., Zaremba, W., Sutskever, I., Bruna, J., Erhan, D., Goodfellow, I. & Fergus, R., 2014, “Intriguing Properties of Neural Networks,” in _Second International Conference on Learning Representations_, Banff, Canada. \[[Available online](https://arxiv.org/pdf/1312.6199.pdf)]
* Hastie, T., Tibshirani, R., & Jerome, F., 2009, _The Elements of Statistical Learning_, in the series _Springer Series in Statistics_, New York: Springer.
* Turing, A., 1950, “Computing Machinery and Intelligence,” _Mind_, LIX: 433–460.
* Turing, A., 1936, “On Computable Numbers with Applications to the Entscheidung-Problem,” _Proceedings of the London Mathematical Society_, 42: 230–265.
* Vilalta, R. & Drissi, Y., 2002, “A Perspective View and Survey of Meta-learning,” _Artificial Intelligence Review_, 18.2:77–95.
* Voronkov, A., 1995, “The Anatomy of Vampire: Implementing Bottom-Up Procedures with Code Trees,” _Journal of Automated Reasoning_, 15.2.
* Wallach, W. & Allen, C., 2010, _Moral Machines: Teaching Robots Right from Wrong,_ Oxford, UK: Oxford University Press.
* Wermter, S. & Sun, R., 2001 (Spring), “The Present and the Future of Hybrid Neural Symbolic Systems: Some Reflections from the Neural Information Processing Systems Workshop,” _AI Magazine_, 22.1: 123–125.
* Suppes, P., 1972, _Axiomatic Set Theory_, New York, NY: Dover.
* Whiteson, S. & Whiteson, D., 2009, “Machine Learning for Event Selection in High Energy Physics,” _Engineering Applications of Artificial Intelligence_ 22.8: 1203–1217.
* Williams, D. E., Hinton G. E., & Williams R. J., 1986 “Learning Representations by Back-propagating Errors,” _Nature_, 323.10: 533–536.
* Winston, P., 1992, _Artificial Intelligence_, Reading, MA: Addison-Wesley.
* Wos, L., Overbeek, R., Lusk R. & Boyle, J., 1992, _Automated Reasoning: Introduction and Applications (2nd edition)_, New York, NY: McGraw-Hill.
* Wos, L., 2013, “The Legacy of a Great Researcher,” in _Automated Reasoning and Mathematics: Essays in Memory of William McCune_, Bonacina, M.P. & Stickel, M.E., eds., 1–14. Berlin: Springer.
* Zalta, E., 1988, _Intensional Logic and the Metaphysics of Intentionality_, Cambridge, MA: Bradford Books.

## Academic Tools

| ![sep man icon](https://plato.stanford.edu/symbols/sepman-icon.jpg) | [How to cite this entry](https://plato.stanford.edu/cgi-bin/encyclopedia/archinfo.cgi?entry=artificial-intelligence).                                                                      |
| ------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| ![sep man icon](https://plato.stanford.edu/symbols/sepman-icon.jpg) | [Preview the PDF version of this entry](https://leibniz.stanford.edu/friends/preview/artificial-intelligence/) at the [Friends of the SEP Society](https://leibniz.stanford.edu/friends/). |
| ![inpho icon](https://plato.stanford.edu/symbols/inpho.png)         | [Look up topics and thinkers related to this entry](https://www.inphoproject.org/entity?sep=artificial-intelligence\&redirect=True) at the Internet Philosophy Ontology Project (InPhO).   |
| ![phil papers icon](https://plato.stanford.edu/symbols/pp.gif)      | [Enhanced bibliography for this entry](http://philpapers.org/sep/artificial-intelligence/) at [PhilPapers](http://philpapers.org/), with links to its database.                            |

## Other Internet Resources

* [Artificial Intelligence Positioned to be a Game-changer](http://www.cbsnews.com/news/60-minutes-artificial-intelligence-charlie-rose-robot-sophia/), an excellent segment on AI from CBS’s esteemed _60 Minutes_ program, this gives a popular science level overview of the current state of AI (as of Ocotober, 2016). The videos in the segment covers applications of AI, Watson’s evolution from winning _Jeopardy!_ to fighting cancer and advances in robotics.
* [MacroVU’s Map Coverage of the Great Debates of AI](http://affect-reason-utility.com/ai/ai.html)
* _AIMA_ textbook:
  * [web site for first edition (1995)](http://www.cs.berkeley.edu/\~russell/aima1e.html)
  * [web site for second edition (2002)](http://aima.cs.berkeley.edu/2nd-ed/)
  * [web site for the third edition (2009)](http://aima.cs.berkeley.edu/)
* [Association for the Advancement of Artificial Intelligence](http://www.aaai.org/)
* [Cognitive Science Society](http://www.cognitivesciencesociety.org/)
* [International Joint Conference on Artificial Intelligence](http://ijcai.org/)
* [Artificial General Intelligence (AGI) Conference](http://www.agi-conference.org/)
* [An introduction and a collection of resources on Artificial General Intelligence](https://sites.google.com/site/narswang/home/agi-introduction)
* [AGI 2010 Workshop Call for a Serious Computational Science of Intelligence](http://agi-conf.org/2010/workshops/#SCSI)

### Cited Resources

* Baydin A.G., Pearlmutter, B. A., Radul, A. A. & Siskind J. M., 2015, “Automatic Differentiation in Machine Learning: A Survey,” arXiv:1502.05767 \[cs.SC]. URL: [\[1502.05767\] Automatic differentiation in machine learning: a survey](http://arxiv.org/abs/1502.05767)
* Benenson, 2016, “[Classification Datasets Results,](http://rodrigob.github.io/are\*we\*there\*yet/build/classification\*datasets\*results.html)” URL = [http://rodrigob.github.io/are\*we\*there\*yet/build/classification\*datasets\*results.html](http://rodrigob.github.io/are\*we\*there\*yet/build/classification\*datasets\*results.html) (Last accessed in July 2018).
* LeCun, Y., Cortes, C. and Burges, C. J.C, 2017, “THE MNIST DATABASE of handwritten digits,” URL = http://yann.lecun.com/exdb/mnist/ (Last accessed in July 2018).
* Levesque, J. H., 2013, “[On Our Best Behaviour](http://www.cs.toronto.edu/\~hector/Papers/ijcai-13-paper.pdf),” _Speech for the IJCAI 2013 Award for Research Excellence_, Beijing.

### Online Courses on AI

1. [Artificial Intelligence: Principles and Techniques](http://web.stanford.edu/class/cs221/) from Stanford
2. [Artificial Intelligence](https://www.edx.org/course/artificial-intelligence-uc-berkeleyx-cs188-1x-0) from Columbia University
3. [Artificial Intelligence](http://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-034-artificial-intelligence-fall-2010/) at MIT (as taugh in Fall 2010)

## Related Entries

[artificial intelligence: logic-based](https://plato.stanford.edu/entries/logic-ai/) | [causation: probabilistic](https://plato.stanford.edu/entries/causation-probabilistic/) | [Chinese room argument](https://plato.stanford.edu/entries/chinese-room/) | [cognitive science](https://plato.stanford.edu/entries/cognitive-science/) | [computability and complexity](https://plato.stanford.edu/entries/computability/) | [computing: modern history of](https://plato.stanford.edu/entries/computing-history/) | [connectionism](https://plato.stanford.edu/entries/connectionism/) | [epistemology: Bayesian](https://plato.stanford.edu/entries/epistemology-bayesian/) | [frame problem](https://plato.stanford.edu/entries/frame-problem/) | [information technology: and moral values](https://plato.stanford.edu/entries/it-moral-values/) | [language of thought hypothesis](https://plato.stanford.edu/entries/language-thought/) | [learning theory, formal](https://plato.stanford.edu/entries/learning-formal/) | [linguistics: computational](https://plato.stanford.edu/entries/computational-linguistics/) | [mind: computational theory of](https://plato.stanford.edu/entries/computational-mind/) | [reasoning: automated](https://plato.stanford.edu/entries/reasoning-automated/) | [reasoning: defeasible](https://plato.stanford.edu/entries/reasoning-defeasible/) | [statistics, philosophy of](https://plato.stanford.edu/entries/statistics/) | [Turing test](https://plato.stanford.edu/entries/turing-test/)

### Acknowledgments

Thanks are due to Peter Norvig and Prentice-Hall for allowing figures from _AIMA_ to be used in this entry. Thanks are due as well to the many first-rate (human) minds who have read earlier drafts of this entry, and provided helpful feedback. Without the support of our AI research and development from both ONR and AFOSR, our knowledge of AI and ML would confessedly be acutely narrow, and we are grateful for the support. We are also very grateful to the anonymous referees who provided us with meticulous reviews in our reviewing round in late 2015 to early 2016. Special acknowledgements are due to the SEP editors and, in particular, Uri Nodelman for patiently working with us throughout and for providing technical and insightful editorial help.

[Copyright © 2018](https://plato.stanford.edu/info.html#c) by\
[Selmer Bringsjord](http://www.rpi.edu/\~brings) <[_Selmer.Bringsjord@gmail.com_](mailto:Selmer%2eBringsjord%40gmail%2ecom)>\
Naveen Sundar Govindarajulu <[_Naveen.Sundar.G@gmail.com_](mailto:Naveen%2eSundar%2eG%40gmail%2ecom)>
